{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51489, 47)\n",
      "(51489, 47)\n",
      "(931453, 47)\n",
      "(929615, 23)\n",
      "Unique ones are:  (3354, 47)\n",
      "Unique ones are:  (3507, 47)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in all files\n",
    "train2015_05_28 = pd.read_csv(\"train2015_05_28_changed.csv\")\n",
    "train2015_06_28 = pd.read_csv(\"train2015_06_28_changed.csv\")\n",
    "train2016_05_28 = pd.read_csv(\"train2016_05_28.csv\")\n",
    "testX = pd.read_csv(\"testX.csv\")\n",
    "\n",
    "print(train2015_05_28.shape)\n",
    "print(train2015_06_28.shape)\n",
    "print(train2016_05_28.shape)\n",
    "print(testX.shape)\n",
    "\n",
    "# the unique ones in either 2015-05-28 or 2015-06-28\n",
    "train2015_05_28_unique = pd.read_csv('train2015_05_28_unique.csv')\n",
    "print(\"Unique ones are: \", train2015_05_28_unique.shape)\n",
    "\n",
    "train2015_06_28_unique = pd.read_csv('train2015_06_28_unique.csv')\n",
    "print(\"Unique ones are: \", train2015_06_28_unique.shape)\n",
    "\n",
    "# read in changed id list\n",
    "#changed_ids = []\n",
    "#with open('changed_ids.txt', 'r') as f:\n",
    "#    for line in f:\n",
    "#        changed_ids.append(int(line.strip()))\n",
    "#print(\"changed list is: \", len(changed_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only 22 products has data. Products 'ind_ahor_fin_ult1' un 'ind_aval_fin_ult1' have not been purchased.\n",
    "- so we need to drop these two columns\n",
    "- in addition, the date information should also be removed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51489, 40)\n",
      "(51489, 40)\n",
      "(931453, 40)\n",
      "Unique ones are:  (3354, 40)\n",
      "Unique ones are:  (3507, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "droplist = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'fecha_dato', 'fecha_alta', 'ult_fec_cli_1t', 'conyuemp', 'tipodom']\n",
    "train2015_05_28.drop(droplist, 1, inplace=True)\n",
    "train2015_06_28.drop(droplist, 1, inplace=True)\n",
    "train2016_05_28.drop(droplist, 1, inplace=True)\n",
    "print(train2015_05_28.shape)\n",
    "print(train2015_06_28.shape)\n",
    "print(train2016_05_28.shape)\n",
    "\n",
    "train2015_05_28_unique.drop(droplist, 1, inplace=True)\n",
    "train2015_06_28_unique.drop(droplist, 1, inplace=True)\n",
    "print(\"Unique ones are: \", train2015_05_28_unique.shape)\n",
    "print(\"Unique ones are: \", train2015_06_28_unique.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that antiguedad (antiquity or Seniority) have negative values -999999, so need to replace it with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51489, 40)\n",
      "(51489, 40)\n",
      "(931453, 40)\n",
      "Unique ones are:  (3354, 40)\n",
      "Unique ones are:  (3507, 40)\n"
     ]
    }
   ],
   "source": [
    "train2015_05_28.loc[train2015_05_28.antiguedad < 0, 'antiguedad'] = 0\n",
    "train2015_06_28.loc[train2015_06_28.antiguedad < 0, 'antiguedad'] = 0\n",
    "train2016_05_28.loc[train2016_05_28.antiguedad < 0, 'antiguedad'] = 0\n",
    "print(train2015_05_28.shape)\n",
    "print(train2015_06_28.shape)\n",
    "print(train2016_05_28.shape)\n",
    "\n",
    "train2015_05_28_unique.loc[train2015_05_28_unique.antiguedad < 0, 'antiguedad'] = 0\n",
    "train2015_06_28_unique.loc[train2015_06_28_unique.antiguedad < 0, 'antiguedad'] = 0\n",
    "print(\"Unique ones are: \", train2015_05_28_unique.shape)\n",
    "print(\"Unique ones are: \", train2015_06_28_unique.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For padding\n",
    "- when compare two different months, some columns values are missing as they are unique to individual months\n",
    "- in order to do the comparison, the month with missing customers will have to pad everything in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding for 55 is:  (3507, 40)\n",
      "2015-05-28 only:  (54996, 40)\n",
      "2015-06-28 only:  (54996, 40)\n",
      "the entire list for 2015:  (58350, 40)\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1','ind_ctma_fin_ult1',\n",
    "               'ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1','ind_dela_fin_ult1',\n",
    "               'ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1','ind_pres_fin_ult1',\n",
    "               'ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    "               'ind_nom_pens_ult1','ind_recibo_ult1']\n",
    "\n",
    "# for padding\n",
    "padding_for_55 = train2015_06_28_unique.copy()\n",
    "padding_for_55[target_cols] = [0] * 22\n",
    "\n",
    "#padding_for_56 = train2015_05_28_unique.copy()\n",
    "#padding_for_56[target_cols] = [0] * 22\n",
    "\n",
    "\n",
    "print(\"padding for 55 is: \", padding_for_55.shape)\n",
    "#print(train2015_06_28_unique[target_cols].iloc[1:2,:])\n",
    "#print(padding_for_55[target_cols].iloc[1:2,:])\n",
    "\n",
    "# compose the changed list for 2015-05-28\n",
    "#train55 = pd.concat([train2015_05_28_unique])\n",
    "train55 = pd.concat([train2015_05_28, padding_for_55])\n",
    "#train55 = pd.concat([train2015_05_28, train2015_05_28_unique, padding_for_55])\n",
    "print(\"2015-05-28 only: \", train55.shape)\n",
    "\n",
    "# compose the changed list for 2015-06-28\n",
    "train56 = pd.concat([train2015_06_28, train2015_06_28_unique])\n",
    "#train56 = pd.concat([train2015_06_28, train2015_06_28_unique, padding_for_56])\n",
    "#train56 = pd.concat([padding_for_56])\n",
    "print(\"2015-06-28 only: \", train56.shape)\n",
    "\n",
    "# changed list for all 2015\n",
    "trainX2015 = pd.concat([train2015_05_28, train2015_05_28_unique, train2015_06_28_unique])\n",
    "print(\"the entire list for 2015: \", trainX2015.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now need to find out changed target in a single column list format\n",
    "\n",
    "trainY = []\n",
    "trainX = []\n",
    "\n",
    "for idx, row in train56.iterrows():\n",
    "    customer_id = row.ncodpers\n",
    "    row55 = train55[train55.ncodpers == customer_id]        \n",
    "    cur_target_list  = row[target_cols].values\n",
    "    prev_target_list = row55[target_cols].values[0]\n",
    "    row55.drop(['ncodpers'], 1, inplace=True)\n",
    "    row55 = row55.values[0]\n",
    "    #print(prev_target_list, \" and \", cur_target_list)\n",
    "\n",
    "    new_target_list =[max(abs(int(x1) - int(x2)), 0) for (x1, x2) in zip(cur_target_list, prev_target_list)]\n",
    "    \n",
    "    for ind, val in enumerate(new_target_list):\n",
    "        if val != 0:\n",
    "            trainX.append(row55)\n",
    "            trainY.append(ind)\n",
    "            \n",
    "print(len(trainY))\n",
    "print(len(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({21: 16094, 0: 14624, 20: 11213, 16: 8840, 19: 8181, 2: 7414, 15: 2942, 9: 2192, 10: 1946, 7: 1278, 4: 874, 5: 789, 17: 598, 11: 522, 6: 465, 14: 318, 3: 161, 8: 98, 12: 84, 13: 42, 18: 23, 1: 15})\n",
      "(78713,)\n",
      "(78713, 41)\n",
      "[  2.30000000e+01   0.00000000e+00   3.40000000e+01   1.00000000e+00\n",
      "   1.00000000e+00   1.00000000e+00   2.80000000e+01   0.00000000e+00\n",
      "   1.78865280e+05   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(trainY))\n",
    "\n",
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)\n",
    "print(trainY.shape)\n",
    "print(trainX.shape)\n",
    "print(trainX[0])\n",
    "\n",
    "trainY.dump(\"trainY.dat\")\n",
    "trainX.dump(\"trainX.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929615, 23)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(931453, 40)\n",
      "(929615, 40)\n",
      "the test ids are:  929615\n",
      "(929615, 39)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# now need to convert testX file\n",
    "testX = pd.read_csv(\"testX.csv\")#, nrows=5)\n",
    "print(testX.shape)\n",
    "print(type(train2016_05_28))\n",
    "print(train2016_05_28.shape)\n",
    "\n",
    "#testX[target_cols] = [0]*22\n",
    "test_ids = testX['ncodpers']\n",
    "testX = train2016_05_28[train2016_05_28.ncodpers.isin(test_ids)]\n",
    "#testX.drop(['ncodpers', 'fecha_dato', 'fecha_alta', 'ult_fec_cli_1t'], 1, inplace=True)\n",
    "\n",
    "print(testX.shape)\n",
    "\n",
    "test_ids = testX['ncodpers']\n",
    "testX.drop(['ncodpers'], 1, inplace=True)\n",
    "print(\"the test ids are: \", len(test_ids))\n",
    "\n",
    "testX = np.array(testX)\n",
    "print(testX.shape)\n",
    "\n",
    "np.save(\"test_ids\", test_ids)\n",
    "np.save(\"testX\", testX)\n",
    "#testX.dump(\"testX.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929615,)\n",
      "0      15889\n",
      "1    1170544\n",
      "2    1170545\n",
      "3    1170547\n",
      "4    1170548\n",
      "Name: ncodpers, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_ids.shape)\n",
    "print(test_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {'seed': 0,\n",
    "          'colsample_bytree': 0.7,\n",
    "          'silent': 1,\n",
    "          'subsample': 0.5,\n",
    "          'learning_rate': 0.1,\n",
    "          'objective': 'binary:logistic',\n",
    "          'max_depth': 10,\n",
    "          'min_child_weight': 100,\n",
    "          'booster': 'gbtree', \n",
    "          'eval_metric': 'mlogloss',\n",
    "         }\n",
    "\n",
    "#params = {'seed': 125,\n",
    "#          'colsample_bytree': 0.7,\n",
    "#          'silent': 1,\n",
    "#          'subsample': 0.7,\n",
    "#          'eta': 0.05,\n",
    "#          'objective': 'multi:softprob',\n",
    "#          'max_depth': 8,\n",
    "#          'min_child_weight': 1,\n",
    "#          'eval_metric': 'mlogloss',\n",
    "#          'num_class' : 22\n",
    "#         }\n",
    "\n",
    "num_rounds = 50\n",
    "\n",
    "trainX = xgb.DMatrix(trainX, label=trainY)\n",
    "testX = xgb.DMatrix(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53979647  0.          0.05524501  0.          0.          0.          0.\n",
      "   0.          0.          0.08326565  0.02992202  0.          0.          0.\n",
      "   0.          0.          0.08116972  0.          0.          0.03603326\n",
      "   0.04820196  0.10860284]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "target_cols = ['ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1','ind_ctma_fin_ult1',\n",
    "               'ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1','ind_dela_fin_ult1',\n",
    "               'ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1','ind_pres_fin_ult1',\n",
    "               'ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    "               'ind_nom_pens_ult1','ind_recibo_ult1']\n",
    "\n",
    "preds = np.load(\"preds500.npy\")\n",
    "preds[preds < 0.02] = 0\n",
    "print(preds[6:7,:])\n",
    "\n",
    "#combined = zip(test_ids, preds)\n",
    "target_cols = np.array(target_cols)\n",
    "    \n",
    "final_preds = [\" \".join(list(target_cols[np.nonzero(pred)])) for pred in preds]    \n",
    "test_ids = np.array(np.load(\"test_ids.npy\"))\n",
    "\n",
    "final_df = pd.DataFrame({'ncodpers':test_ids, 'added_products':final_preds})\n",
    "final_df.to_csv(\"final_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
