{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only 22 products has data. Products 'ind_ahor_fin_ult1' un 'ind_aval_fin_ult1' have not been purchased.\n",
    "- so we need to drop these two columns\n",
    "- in addition, the date information should also be removed!\n",
    "#### It seems that antiguedad (antiquity or Seniority) have negative values -999999, so need to replace it with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "target_cols = ['ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1','ind_ctma_fin_ult1',\n",
    "               'ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1','ind_dela_fin_ult1',\n",
    "               'ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1','ind_pres_fin_ult1',\n",
    "               'ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    "               'ind_nom_pens_ult1','ind_recibo_ult1']\n",
    "\n",
    "droplist = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1']\n",
    "#droplist = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'fecha_dato', 'fecha_alta', 'ult_fec_cli_1t', 'conyuemp', 'tipodom']\n",
    "\n",
    "def fetch_and_reduce_data(file1, file2, file_id):\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    ids = np.load(file_id)\n",
    "    \n",
    "    df1 = df1[df1.ncodpers.isin(ids)]\n",
    "    df2 = df2[df2.ncodpers.isin(ids)]\n",
    "    \n",
    "    # clean data\n",
    "    df1.drop(droplist, 1, inplace=True)\n",
    "    df1.loc[df1.antiguedad < 0, 'antiguedad'] = 0\n",
    "    \n",
    "    df2.drop(droplist, 1, inplace=True)\n",
    "    df2.loc[df2.antiguedad < 0, 'antiguedad'] = 0\n",
    "    \n",
    "    df1.to_csv(\"changed_\"+file1, index=False)\n",
    "    df2.to_csv(\"changed_\"+file2, index=False)\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "def padding(df1, df2, df2_unique):\n",
    "    df1_padding = df2_unique.copy()\n",
    "    df1_padding[target_cols] = [0]* 22\n",
    "    \n",
    "    df1 = pd.concat([df1, df1_padding])\n",
    "    df2 = pd.concat([df2, df2_unique])\n",
    "    print(\"df2 shape is \", df2.shape)\n",
    "    return df1, df2\n",
    "\n",
    "def get_data_and_labels(df1, df2, num):\n",
    "    trainY = []\n",
    "    trainX = []\n",
    "\n",
    "    for idx, row2 in df2.iterrows():\n",
    "        customer_id = row2.ncodpers\n",
    "        row1 = df1[df1.ncodpers == customer_id]        \n",
    "        cur_target_list  = row2[target_cols].values\n",
    "        prev_target_list = row1[target_cols].values[0]\n",
    "        row1.drop(['ncodpers'], 1, inplace=True)\n",
    "        row1 = row1.values[0]\n",
    "    \n",
    "        new_target_list =[max(int(x1) - int(x2), 0) for (x1, x2) in zip(cur_target_list, prev_target_list)]\n",
    "    \n",
    "        for ind, val in enumerate(new_target_list):\n",
    "            if val != 0:\n",
    "                trainX.append(row1)\n",
    "                trainY.append(ind)\n",
    "    \n",
    "    print(Counter(trainY))\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    print(trainY.shape)\n",
    "    print(trainX.shape)\n",
    "    #print(trainX[0])\n",
    "\n",
    "    trainY.dump(\"trainY\"+num+\".dat\")\n",
    "    trainX.dump(\"trainX\"+num+\".dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(931453, 47)\n",
      "(929615, 23)\n",
      "(53655, 45)\n",
      "(53655, 45)\n",
      "(45006, 45)\n",
      "(45006, 45)\n",
      "(33318, 45)\n",
      "(33318, 45)\n",
      "Unique ones are:  (3507, 45)\n",
      "Unique ones are:  (266215, 45)\n",
      "Unique ones are:  (279153, 45)\n"
     ]
    }
   ],
   "source": [
    "# read in all files, and test file are the same throughout\n",
    "train2016_05_28 = pd.read_csv(\"train2016_05_28.csv\")\n",
    "testX = pd.read_csv(\"testX.csv\")\n",
    "print(train2016_05_28.shape)\n",
    "print(testX.shape)\n",
    "\n",
    "# for lag training\n",
    "train2015_05_28_lag, train2015_10_28_lag = fetch_and_reduce_data(\n",
    "    'train2015_05_28_lag.csv', 'train2015_10_28_lag.csv', 'changed_ids0510.npy')\n",
    "print(train2015_05_28_lag.shape)\n",
    "print(train2015_10_28_lag.shape)\n",
    "\n",
    "train2015_06_28_lag, train2015_11_28_lag = fetch_and_reduce_data(\n",
    "    'train2015_06_28_lag.csv', 'train2015_11_28_lag.csv', 'changed_ids0611.npy')\n",
    "print(train2015_06_28_lag.shape)\n",
    "print(train2015_11_28_lag.shape)\n",
    "\n",
    "# for final training\n",
    "train2015_05_28, train2015_06_28 = fetch_and_reduce_data(\n",
    "    'train2015_05_28.csv', 'train2015_06_28.csv', 'changed_ids0506.npy')\n",
    "print(train2015_05_28.shape)\n",
    "print(train2015_06_28.shape)\n",
    "\n",
    "\n",
    "# the unique ones in 2015-05-28, 2015-10-28 and 2015-11-28\n",
    "train2015_06_28_unique = pd.read_csv('train2015_06_28_unique.csv')\n",
    "train2015_06_28_unique.drop(droplist, 1, inplace=True)\n",
    "train2015_06_28_unique.loc[train2015_06_28_unique.antiguedad < 0, 'antiguedad'] = 0\n",
    "print(\"Unique ones are: \", train2015_06_28_unique.shape)\n",
    "\n",
    "train2015_10_28_lag_unique = pd.read_csv('train2015_10_28_lag_unique.csv')\n",
    "train2015_10_28_lag_unique.drop(droplist, 1, inplace=True)\n",
    "train2015_10_28_lag_unique.loc[train2015_10_28_lag_unique.antiguedad < 0, 'antiguedad'] = 0\n",
    "print(\"Unique ones are: \", train2015_10_28_lag_unique.shape)\n",
    "\n",
    "train2015_11_28_lag_unique = pd.read_csv('train2015_11_28_lag_unique.csv')\n",
    "train2015_11_28_lag_unique.drop(droplist, 1, inplace=True)\n",
    "train2015_11_28_lag_unique.loc[train2015_11_28_lag_unique.antiguedad < 0, 'antiguedad'] = 0\n",
    "print(\"Unique ones are: \", train2015_11_28_lag_unique.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For padding\n",
    "- when compare two different months, some columns values are missing as they are unique to individual months\n",
    "- in order to do the comparison, the month with missing customers will have to pad everything in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 shape is  (36825, 45)\n",
      "df2 shape is  (319870, 45)\n",
      "df2 shape is  (324159, 45)\n"
     ]
    }
   ],
   "source": [
    "# for padding\n",
    "train55, train56 = padding(train2015_05_28, train2015_06_28, train2015_06_28_unique)\n",
    "\n",
    "train55_lag, train10_lag = padding(train2015_05_28_lag, train2015_10_28_lag, train2015_10_28_lag_unique)\n",
    "\n",
    "train56_lag, train11_lag = padding(train2015_06_28_lag, train2015_11_28_lag, train2015_11_28_lag_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 9457, 21: 9131, 20: 8229, 19: 5161, 16: 4755, 15: 2942, 2: 1934, 10: 1219, 9: 1085, 7: 503, 4: 349, 11: 246, 5: 222, 17: 159, 6: 154, 3: 55, 8: 33, 13: 21, 1: 9, 14: 8, 12: 4, 18: 3})\n",
      "(45679,)\n",
      "(45679, 44)\n"
     ]
    }
   ],
   "source": [
    "# now need to find out changed target in a single column list format\n",
    "get_data_and_labels(train55, train56, '0506')\n",
    "get_data_and_labels(train55_lag, train10_lag, '0510')\n",
    "get_data_and_labels(train56_lag, train11_lag, '0511')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929615, 23)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(931453, 45)\n",
      "(929615, 45)\n",
      "the test ids are:  929615\n",
      "(929615, 44)\n",
      "1838\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# now need to convert testX file\n",
    "testX = pd.read_csv(\"testX.csv\")#, nrows=5)\n",
    "print(testX.shape)\n",
    "print(type(train2016_05_28))\n",
    "print(train2016_05_28.shape)\n",
    "test_ids1 = train2016_05_28['ncodpers']   #931453\n",
    "\n",
    "#testX[target_cols] = [0]*22\n",
    "test_ids2 = testX['ncodpers']   #929615\n",
    "testX = train2016_05_28[train2016_05_28.ncodpers.isin(test_ids2)]\n",
    "#testX.drop(['ncodpers', 'fecha_dato', 'fecha_alta', 'ult_fec_cli_1t'], 1, inplace=True)\n",
    "\n",
    "print(testX.shape)\n",
    "testX.drop(['ncodpers'], 1, inplace=True)\n",
    "print(\"the test ids are: \", len(test_ids2))\n",
    "\n",
    "testX = np.array(testX)\n",
    "print(testX.shape)\n",
    "\n",
    "np.save(\"test_ids\", test_ids2)\n",
    "np.save(\"testX\", testX)\n",
    "#testX.dump(\"testX.dat\")\n",
    "\n",
    "unique_test_ids = list(set(test_ids1) - set(test_ids2))\n",
    "print(len(unique_test_ids))\n",
    "np.save(\"unique_test_ids\", unique_test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_in_test_unique shape is (16815, 48)\n"
     ]
    }
   ],
   "source": [
    "# let's see if any of 1838 have been defined earlier\n",
    "train = pd.read_csv(\"train_ver2.csv\")\n",
    "train_in_test_unique = train[train.ncodpers.isin(unique_test_ids)]\n",
    "print(\"train_in_test_unique shape is\", train_in_test_unique.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {'seed': 0,\n",
    "          'colsample_bytree': 0.7,\n",
    "          'silent': 1,\n",
    "          'subsample': 0.5,\n",
    "          'learning_rate': 0.1,\n",
    "          'objective': 'binary:logistic',\n",
    "          'max_depth': 10,\n",
    "          'min_child_weight': 100,\n",
    "          'booster': 'gbtree', \n",
    "          'eval_metric': 'mlogloss',\n",
    "         }\n",
    "\n",
    "#params = {'seed': 125,\n",
    "#          'colsample_bytree': 0.7,\n",
    "#          'silent': 1,\n",
    "#          'subsample': 0.7,\n",
    "#          'eta': 0.05,\n",
    "#          'objective': 'multi:softprob',\n",
    "#          'max_depth': 8,\n",
    "#          'min_child_weight': 1,\n",
    "#          'eval_metric': 'mlogloss',\n",
    "#          'num_class' : 22\n",
    "#         }\n",
    "\n",
    "num_rounds = 50\n",
    "\n",
    "trainX = xgb.DMatrix(trainX, label=trainY)\n",
    "testX = xgb.DMatrix(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53979647  0.          0.05524501  0.          0.          0.          0.\n",
      "   0.          0.          0.08326565  0.02992202  0.          0.          0.\n",
      "   0.          0.          0.08116972  0.          0.          0.03603326\n",
      "   0.04820196  0.10860284]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "target_cols = ['ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1','ind_ctma_fin_ult1',\n",
    "               'ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1','ind_dela_fin_ult1',\n",
    "               'ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1','ind_pres_fin_ult1',\n",
    "               'ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    "               'ind_nom_pens_ult1','ind_recibo_ult1']\n",
    "\n",
    "preds = np.load(\"preds500.npy\")\n",
    "preds[preds < 0.02] = 0\n",
    "print(preds[6:7,:])\n",
    "\n",
    "#combined = zip(test_ids, preds)\n",
    "target_cols = np.array(target_cols)\n",
    "    \n",
    "final_preds = [\" \".join(list(target_cols[np.nonzero(pred)])) for pred in preds]    \n",
    "test_ids = np.array(np.load(\"test_ids.npy\"))\n",
    "\n",
    "final_df = pd.DataFrame({'ncodpers':test_ids, 'added_products':final_preds})\n",
    "final_df.to_csv(\"final_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
