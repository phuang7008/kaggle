{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data in most data analysis are huge, single run might bring down the whole machine. Keras provides a way to batch process data using its train_on_batch feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "# define bactch_generator function. This code was copied from Chenglong Chen at\n",
    "# https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices\n",
    "# Note: the data type for X and y have to be <class 'scipy.sparse.csr.csr_matrix'>\n",
    "# it won't work for pd.DataFrame or np.array objects\n",
    "def batch_generator(X=None, y=None, batch_size=128, shuffle=False):\n",
    "    number_of_batches = int(X.shape[0] / batch_size)\n",
    "    print(number_of_batches)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "        print(\"shuffled!\")\n",
    "        \n",
    "    while True:\n",
    "        batch_index = sample_index[counter*batch_size:(counter+1)*batch_size]\n",
    "        print(batch_index[2:5,])\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        \n",
    "        if y is not None:\n",
    "            y_batch = y[batch_index]\n",
    "            yield X_batch, y_batch\n",
    "        else:\n",
    "            yield X_batch\n",
    "        \n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "            \n",
    "## neural net\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(400, input_dim = xtrain.shape[1], init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "        \n",
    "    model.add(Dense(200, init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, init = 'he_normal'))\n",
    "    model.compile(loss = 'mae', optimizer = 'adadelta')\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes are used for testing:\n",
    "It seems keras needs theano. And theano only run on python 3.4 not 3.5 in the window's system, so I have to downgrade my python. In anaconda, I run the following: conda install python=3.4.5 for downgrading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 132)\n",
      "True\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(30000,)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n",
      "sparse data type is  <class 'list'>\n",
      "[<60000x2 sparse matrix of type '<class 'numpy.uint8'>'\n",
      "\twith 60000 stored elements in Compressed Sparse Row format>, <60000x2 sparse matrix of type '<class 'numpy.uint8'>'\n",
      "\twith 60000 stored elements in Compressed Sparse Row format>, <60000x2 sparse matrix of type '<class 'numpy.uint8'>'\n",
      "\twith 60000 stored elements in Compressed Sparse Row format>]\n",
      "116\n",
      "117\n",
      "Dim of xtrain:  (30000, 1062)\n",
      "Dim of xtest:  (30000, 1062)\n",
      "type of xtrain is: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "sklearn.cross_validation.KFold(n=30000, n_folds=10, shuffle=False, random_state=111)\n",
      "Training:  [ 3000  3001  3002 ..., 29997 29998 29999] 27000\n",
      "testing: [   0    1    2 ..., 2997 2998 2999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [3000 3001 3002 ..., 5997 5998 5999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [6000 6001 6002 ..., 8997 8998 8999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [ 9000  9001  9002 ..., 11997 11998 11999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [12000 12001 12002 ..., 14997 14998 14999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [15000 15001 15002 ..., 17997 17998 17999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [18000 18001 18002 ..., 20997 20998 20999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [21000 21001 21002 ..., 23997 23998 23999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 29997 29998 29999] 27000\n",
      "testing: [24000 24001 24002 ..., 26997 26998 26999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Training:  [    0     1     2 ..., 26997 26998 26999] 27000\n",
      "testing: [27000 27001 27002 ..., 29997 29998 29999] 3000\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "os.chdir(r\"J:\\Tutorial\\Kaggle Competitions\\AllState\")\n",
    "\n",
    "# read data in. the first column like index, I could use index_col=0 to\n",
    "# use the first column as index. However, it is not incremental equally\n",
    "# the order is 1, 2, 5, 10; not like 1,2,3,4,5. So after shuffle, you will\n",
    "# get out of bound error!\n",
    "train = pd.read_csv('train.csv', nrows=30000)\n",
    "test  = pd.read_csv('test.csv', nrows=30000)\n",
    "print(train.shape)\n",
    "print(isinstance(train, pd.DataFrame))\n",
    "#print(test.head())\n",
    "\n",
    "row_index = list(train.index)\n",
    "np.random.shuffle(row_index)\n",
    "train = train.iloc[row_index]\n",
    "#print(isinstance(train, pd.DataFrame))\n",
    "print(type(train))\n",
    "\n",
    "## set test loss to NaN\n",
    "test['loss'] = np.nan\n",
    "\n",
    "## response (or targets)\n",
    "y = np.log(train['loss'].values+200)\n",
    "print(train['loss'].shape)\n",
    "\n",
    "# IDs\n",
    "train_id = train['id'].values   # this is np.ndarray with .values\n",
    "test_id = test['id'].values\n",
    "print(type(train['id']))        # this is pd.Series without .values\n",
    "print(type(train['id'].values))\n",
    "\n",
    "# combine train and test by stacking\n",
    "num_of_train = train.shape[0]\n",
    "allData = pd.concat((train, test), axis=0)\n",
    "\n",
    "# preprocessing and transform to sparse matrix\n",
    "sparse_data = []\n",
    "print(\"sparse data type is \", type(sparse_data))\n",
    "\n",
    "f_cat = [f for f in allData.columns if 'cat' in f ]\n",
    "#print(allData.columns)\n",
    "for f in f_cat:\n",
    "    dummy = pd.get_dummies(allData[f].astype('category'))\n",
    "    tmp = csr_matrix(dummy)\n",
    "    sparse_data.append(tmp)\n",
    "print(sparse_data[2:5])\n",
    "print(len(sparse_data))\n",
    "\n",
    "f_cont = [f for f in allData.columns if 'cont' in f]\n",
    "scaler = StandardScaler()\n",
    "tmp = csr_matrix(scaler.fit_transform(allData[f_cont]))\n",
    "sparse_data.append(tmp)\n",
    "print(len(sparse_data))\n",
    "del(allData, train, test)\n",
    "\n",
    "# sparse train and test data\n",
    "xData = hstack(sparse_data, format='csr')\n",
    "xtrain = xData[:num_of_train,:]\n",
    "xtest  = xData[num_of_train:,:]\n",
    "print(\"Dim of xtrain: \", xtrain.shape)\n",
    "print(\"Dim of xtest: \", xtest.shape)\n",
    "print(\"type of xtrain is:\", type(xtrain))\n",
    "\n",
    "## cv-folds: 'folds' is actually a generator, \n",
    "# which doesn't compute the train-test split until it is needed.\n",
    "# Note: only one fold is left out for testing.\n",
    "nfolds = 10\n",
    "folds = KFold(len(y), n_folds = nfolds, shuffle = False, random_state = 111)\n",
    "print(folds)\n",
    "\n",
    "## train models\n",
    "i = 0\n",
    "nbags = 10\n",
    "nepochs = 55\n",
    "pred_oob = np.zeros(xtrain.shape[0])\n",
    "pred_test = np.zeros(xtest.shape[0])\n",
    "\n",
    "for (inTr, inTe) in folds:\n",
    "    print(\"Training: \", inTr, len(inTr))\n",
    "    print(\"testing:\", inTe, len(inTe))\n",
    "    xtr = xtrain[inTr]\n",
    "    ytr = y[inTr]\n",
    "    xte = xtrain[inTe]\n",
    "    yte = y[inTe]\n",
    "    pred = np.zeros(xte.shape[0])\n",
    "    aa = np.arange(10)\n",
    "    print(aa)\n",
    "    print(type(xtr[aa,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
