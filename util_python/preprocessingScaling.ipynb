{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In data science, we need to conduct normalization or standardization for every numeric data variables (features).\n",
    "- scipy.sparse matrices include: Compressed Sparse Rows (scipy.sparse.csr_matrix) and Compressed Sparse Columns format (scipy.sparse.csc_matrix). Any other sparse input will be converted to the Compressed Sparse Rows representation.\n",
    "- If the centered data is expected to be small enough, explicitly converting the input to an array using the toarray() method of sparse matrices is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[-144.93189226  213.45502692 -159.49253106]\n",
      " [-139.42116928  179.02777073 -129.00901949]\n",
      " [  16.71490058  164.14533923 -160.13514532]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "\n",
    "# gererator some data to work with\n",
    "X, y = make_classification(n_samples=300, \n",
    "                           n_features=3, \n",
    "                           n_redundant=0, \n",
    "                           n_informative=2, \n",
    "                           random_state=22, \n",
    "                           n_clusters_per_class=1,\n",
    "                           scale=100)\n",
    "print(type(X))\n",
    "print(X[2:5],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In sklearn, there are several different scaling (normalization) methods. \n",
    "#### first, without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy without scaling is:  0.416666666667\n"
     ]
    }
   ],
   "source": [
    "# sklearn.cross_validation will be deprecated. Thus, I use model_selection\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "# without normalization\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf = SVC()\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"The prediction accuracy without scaling is: \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization using various scaling methods inside preprocessing package\n",
    "#### default scaling: scale() or StandardScaler()  [they are the same thing!!!]\n",
    "#### formula: (x - mean)/std\n",
    "- The utility class StandardScaler() that implements the Transformer API [such as fit(), transform() and fit_transform()] to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set.\n",
    "- The scale() function combines all the above operations within a single step, but you will not be able to do the same transformation in test dataset as in training dataset\n",
    "- They can handle scipy.sparse matrices as long as with_mean=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.34037378 -1.72873468 -0.63109555]\n",
      " [ 0.71884974  0.77980568 -1.17636869]]\n",
      "[ -4.14483263e-17  -2.77925830e-16   1.85037171e-17]\n",
      "[ 1.  1.  1.]\n",
      "Using default scaling:  0.966666666667\n",
      "Mean:  [   7.78519807  103.66603519   -0.80587646]\n",
      "Scale_: [ 102.534033     71.65868856  134.2951084 ]\n",
      "Using StandardScaler():  0.933333333333\n"
     ]
    }
   ],
   "source": [
    "# With normalization (Scaling)\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "# Using default scaling using scale():\n",
    "Xc = preprocessing.scale(Xc)\n",
    "print(Xc[0:2,])\n",
    "print(Xc.mean(axis=0))\n",
    "print(Xc.std(axis=0))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Using default scaling: \", clf.score(X_test, y_test))\n",
    "\n",
    "###################################################\n",
    "# using default scaling through StandardScaler():\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "tmp_scaler = preprocessing.StandardScaler()\n",
    "tmp_scaler = tmp_scaler.fit(Xc)\n",
    "print(\"Mean: \", tmp_scaler.mean_)\n",
    "# print(\"Standard Deviation: \", tmp_scaler.std_)  \n",
    "# std_ will be deprecated, use scale_ instead\n",
    "print(\"Scale_:\", tmp_scaler.scale_)\n",
    "Xc = tmp_scaler.transform(Xc)\n",
    "\n",
    "# Here y is not continuous data (discrete), so we will use classification\n",
    "# the loss function will be one of [MAE, MSE, RMSE etc] if y is continuous\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Using StandardScaler(): \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScaler(): [minmax_scale() is the single step version]\n",
    "Transforms features by scaling each feature to a given range.\n",
    "The transformation is given by:\n",
    "- X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "- X_scaled = X_std * (max - min) + min\n",
    "- where min, max = feature_range(min,max), default=(0, 1).\n",
    "- This transformation is often used as an alternative to zero mean, unit variance scaling.\n",
    "The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.43917499 -0.46885612]\n",
      " [ 0.11032647  0.31086199 -0.71041624]\n",
      " [-0.63910267  0.535796   -0.71274547]\n",
      " [-0.62086298  0.39214947 -0.61218764]\n",
      " [-0.10407545  0.33005304 -0.7148653 ]]\n",
      "Using minmax_scaling:  0.95\n",
      "Using MinMaxScaler():  0.916666666667\n"
     ]
    }
   ],
   "source": [
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "# minmax_scale()\n",
    "Xc = preprocessing.minmax_scale(X, feature_range=(-1,1))\n",
    "print(Xc[0:5:,])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Using minmax_scaling: \", clf.score(X_test, y_test))\n",
    "\n",
    "################################################\n",
    "# MinMaxScaler()\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "mm_scaler = preprocessing.MinMaxScaler(feature_range=(-2,2))\n",
    "Xc = mm_scaler.fit_transform(Xc)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Using MinMaxScaler(): \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxAbsScaler() and maxabs_scale() for Scaling sparse data\n",
    "Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it makes sense to scale sparse inputs, especially if features are on different scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41375162  0.65737298 -0.44338447]\n",
      " [-0.3980196   0.55134808 -0.35864122]\n",
      " [ 0.0477177   0.50551496 -0.44517092]]\n",
      "using maxabs_scale(): 0.95\n",
      "[[-0.41375162  0.65737298 -0.44338447]\n",
      " [-0.3980196   0.55134808 -0.35864122]\n",
      " [ 0.0477177   0.50551496 -0.44517092]]\n",
      "using MaxAbsScaler():  0.866666666667\n"
     ]
    }
   ],
   "source": [
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "# maxabs_scale()\n",
    "Xc = preprocessing.maxabs_scale(Xc)\n",
    "print(Xc[2:5,:])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"using maxabs_scale():\", clf.score(X_test, y_test))\n",
    "\n",
    "############################################################\n",
    "# MaxAbsScaler()\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "ma_scaler = preprocessing.MaxAbsScaler()\n",
    "Xc = ma_scaler.fit_transform(Xc)\n",
    "print(Xc[2:5, :])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"using MaxAbsScaler(): \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling data with outliers\n",
    "use robust_scale() and RobustScaler() as drop-in replacements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.09751646  1.44361657 -0.59836839]\n",
      " [-1.05923922  1.00427368 -0.45472651]\n",
      " [ 0.02527523  0.81435176 -0.60139647]]\n",
      "using robust_scale():  0.983333333333\n",
      "[[-1.09751646  1.44361657 -0.59836839]\n",
      " [-1.05923922  1.00427368 -0.45472651]\n",
      " [ 0.02527523  0.81435176 -0.60139647]]\n",
      "using RobustScaler():  0.966666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# robust_scale()\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "Xc = preprocessing.robust_scale(Xc)\n",
    "print(Xc[2:5,])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"using robust_scale(): \", clf.score(X_test, y_test))\n",
    "\n",
    "###################################################################\n",
    "# RobustScaler\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "r_scaler = preprocessing.RobustScaler()\n",
    "Xc = r_scaler.fit_transform(Xc)\n",
    "print(Xc[2:5,])\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"using RobustScaler(): \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization\n",
    "Use a threshold value to set feature values to either 0 or 1 (or boolean value).\n",
    "Just like others, it has two forms: binarize() and Binarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "# binarize()\n",
    "Xc = preprocessing.binarize(Xc, threshold=50)\n",
    "print(Xc[2:5,])\n",
    "\n",
    "#########################################################\n",
    "# Binarizer()\n",
    "Xc = X.copy()\n",
    "yc = y.copy()\n",
    "\n",
    "binarizer = preprocessing.Binarizer(threshold=2.0)\n",
    "Xc = binarizer.fit_transform(Xc)\n",
    "print(Xc[2:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "It is used to scale individual SAMPLEs to have unit norm. The general formula is: (each individual data from each row) / norm(row). Here the norm(row) is the square root of the sum of square of each individual data from this specific row (this is L2 norm). you could also use L1 norms.\n",
    "- There are two forms available just like others\n",
    "- normalize() and Normalizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using L1 norm: [[-0.27985643  0.41217126 -0.30797231]\n",
      " [-0.31158496  0.40009964 -0.2883154 ]]\n",
      "Using L2 norm [[-0.47781028  0.70371679 -0.52581367]\n",
      " [-0.53413552  0.6858721  -0.4942456 ]]\n",
      "Using default norm [[-0.47781028  0.70371679 -0.52581367]\n",
      " [-0.53413552  0.6858721  -0.4942456 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "Xc = X.copy()\n",
    "\n",
    "# use L1 norm\n",
    "X_normalized = preprocessing.normalize(Xc, norm='l1')\n",
    "print(\"using L1 norm:\", X_normalized[2:4,])\n",
    "\n",
    "# use L2 norm\n",
    "X_normalized = preprocessing.normalize(Xc, norm='l2')\n",
    "print(\"Using L2 norm\", X_normalized[2:4,])\n",
    "\n",
    "# using default norm(row)\n",
    "X_normalized = preprocessing.normalize(Xc)\n",
    "print(\"Using default norm\", X_normalized[2:4,])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
