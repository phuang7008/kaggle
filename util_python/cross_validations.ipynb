{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation is an important part of machine learning. There are different ways for cross validation. \n",
    "We can use it to evaluate:\n",
    "- what models are more effective\n",
    "- what parameters to use for a specific model\n",
    "- selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# note: sklearn.cross_validation import train_test_split will be deprecated\n",
    "# note: sklearn.cross_validation import cross_val_score will be deprecated\n",
    "# note: sklearn.cross_validation import KFold will be deprecated\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import math, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation Metrics\n",
    "In order to evaluate each model system, we need to have metrics systems to help us. \n",
    "- for classification: the target(s) are category data, so we use ***metrics.accuracy_score*** for measuring\n",
    "  * **error** - binary classification error rate. It is calculated as # (wrong cases) / #(all casees). Treat predicted values with probability p > 0.5 as positive\n",
    "  * **merror** - multiclass classification error rate. It is calculated as # (wrong cases) / #(all casees).\n",
    "- for regression: the target(s) are continuous data. The goal is to ___minimize___ them in the loss functions:\n",
    "  * **Mean Absolute Error (MAE): metrics.mean_absolute_error** \n",
    "  $$mae = \\frac{1}{n}\\sum_{i=0}^n|y_{i} - \\bar{y}_{i}|$$\n",
    "  * **Mean Square Error (MSE): metics.mean_squared_error **\n",
    "  $$mse = \\frac{1}{n}\\sum_{i=0}^n(y_{i} - \\bar{y}_{i})^2$$\n",
    "  * **Root Mean Square Error (RMSE) **\n",
    "  $$rmse = \\sqrt{\\frac{1}{n}\\sum_{i=0}^n(y_{i} - \\bar{y}_{i})^2}$$\n",
    "  * **Logloss** - negaive log-likelihood\n",
    "  * **AUC**  - area under curve\n",
    "  * **NDCG** - normalized discounted cumulative gain\n",
    "  * **MAP**  - mean average precision\n",
    "- by default, an error metric will be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without cross_validation\n",
    "- run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model score is: 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "y = load_iris().target\n",
    "\n",
    "# preprocessing data and split it into train and test sets\n",
    "X = preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# model\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The model score is:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do train_test_split, part of the samples are used for testing. However, it provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy!\n",
    "\n",
    "How to make use of those test data for training ===> K-folds cross_validation would solve this problem:\n",
    "- A model is trained using k-1 of the folds as training data;\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "- The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop (using different test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.2581988897471611, 0.0, 0.2581988897471611, 0.4472135954999579, 0.2581988897471611, 0.0, 0.0, 0.0]\n",
      "0.349544026518\n"
     ]
    }
   ],
   "source": [
    "# Here is the step by step cross validation (cv)\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "# cv fold\n",
    "nfolds = 10\n",
    "kf = KFold(n_splits=nfolds, shuffle=True, random_state=int(time.time()))\n",
    "\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "rmse = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"%s, %s\" % (train_index, test_index))\n",
    "    #print(\"%d, %d\" % (len(train_index), len(test_index)))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    rmse.append(math.sqrt(metrics.mean_squared_error(y_pred, y_test)))\n",
    "\n",
    "print(rmse)\n",
    "print(np.sqrt(np.mean(rmse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Good** thing is that you usually don't need to inplement the details about cross validation. The sklearn package provides a high level function ***cross_val_score()*** to do all the above.\n",
    "- In addition, for classification problems, ***stratified sampling*** is recommended for creating the folds; that is\n",
    "  * each response (or target) should be represented with equal proportions in each of the K folds.\n",
    "  * **sklearn.cross_val_score()** function does this by default!\n",
    "- Validation options are:\n",
    "    - ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.93333333  1.          0.93333333  1.          0.93333333\n",
      "  0.86666667  1.          1.          1.        ]\n",
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "# Here is the simplified version of cross validation (cv)\n",
    "\n",
    "X = load_iris().data\n",
    "X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "#scores = -cross_val_score(svm.SVC(), X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "scores = cross_val_score(SVC(), X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use this to tune model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95999999999999996, 0.95333333333333337, 0.96666666666666656, 0.96666666666666656, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.98000000000000009, 0.96666666666666656, 0.96666666666666656, 0.97333333333333338, 0.95999999999999996, 0.96666666666666656, 0.95999999999999996, 0.96666666666666656, 0.95333333333333337, 0.95333333333333337, 0.95333333333333337]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f89e45d0b70>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFkCAYAAACJu/k0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89e4bd4c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "#X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "k_range = range(1,31)\n",
    "k_scores = []\n",
    "\n",
    "for i in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    k_scores.append(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n",
    "    \n",
    "print(k_scores)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, k_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The following will be done in XGBoost\n",
    "- the dataset will be taken from Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.042831\ttrain-error:0.046522\n",
      "[1]\ttest-error:0.039727\ttrain-error:0.043605\n",
      "[2]\ttest-error:0.024829\ttrain-error:0.023338\n",
      "[3]\ttest-error:0.027312\ttrain-error:0.028251\n",
      "[4]\ttest-error:0.011794\ttrain-error:0.011823\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from pprint import pprint\n",
    "\n",
    "# for reproducibility, if you don't want this, you could use time.time() to get different value every time\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "dtrain = xgb.DMatrix('../../data/agaricus.txt.train')\n",
    "dtest  = xgb.DMatrix('../../data/agaricus.txt.test')\n",
    "\n",
    "# train parameters - we are going to use 5 decision tree stumps with average learning rate.\n",
    "# the defaul error metric is 'error'\n",
    "params = {'objective' : 'binary:logistic',\n",
    "          'max_depth' : 2,\n",
    "          'silent' : 1,\n",
    "          'eta' : 0.5}\n",
    "num_rounds = 5\n",
    "watch_list = [(dtest, 'test'), (dtrain, 'train')]\n",
    "\n",
    "# training\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's change the error metric to logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.377093\ttrain-logloss:0.380598\n",
      "[1]\ttest-logloss:0.245811\ttrain-logloss:0.247331\n",
      "[2]\ttest-logloss:0.174941\ttrain-logloss:0.175047\n",
      "[3]\ttest-logloss:0.124244\ttrain-logloss:0.122301\n",
      "[4]\ttest-logloss:0.089699\ttrain-logloss:0.089889\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = 'logloss'\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we could use multiple error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-auc:0.960373\ttest-map:0.98915\ttrain-auc:0.958228\ttrain-map:0.966873\n",
      "[1]\ttest-auc:0.992342\ttest-map:0.991732\ttrain-auc:0.99115\ttrain-map:0.989131\n",
      "[2]\ttest-auc:0.996324\ttest-map:0.995753\ttrain-auc:0.995134\ttrain-map:0.995014\n",
      "[3]\ttest-auc:0.998298\ttest-map:0.998303\ttrain-auc:0.99819\ttrain-map:0.998176\n",
      "[4]\ttest-auc:0.999339\ttest-map:0.999292\ttrain-auc:0.999085\ttrain-map:0.999081\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = ['auc', 'map']\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom evaluation metric\n",
    "In order to create our own evaluation metric, the only thing needed to do is to create a method taking two arguments - ***predicted probabilities*** and ***DMatrix*** objet holding training data\n",
    "\n",
    "In this example, our classification metric will simply count the number of mis-classified examples assuming that classes with p > 0.5 are positive. You can change this threshold if you want more certainty\n",
    "\n",
    "The algorithm is getting better when the number of mis-classified examples is getting lower. Remember to also set the argument ***maximize=False*** while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.042831\ttrain-error:0.046522\ttest-misclassified:69\ttrain-misclassified:303\n",
      "[1]\ttest-error:0.039727\ttrain-error:0.043605\ttest-misclassified:64\ttrain-misclassified:284\n",
      "[2]\ttest-error:0.024829\ttrain-error:0.023338\ttest-misclassified:40\ttrain-misclassified:152\n",
      "[3]\ttest-error:0.027312\ttrain-error:0.028251\ttest-misclassified:44\ttrain-misclassified:184\n",
      "[4]\ttest-error:0.011794\ttrain-error:0.011823\ttest-misclassified:19\ttrain-misclassified:77\n"
     ]
    }
   ],
   "source": [
    "def misclassified(pred_probs, dtrain):\n",
    "    labels = dtrain.get_label()   # obtain true labels\n",
    "    preds  = pred_probs > 0.5     # obtain predicted values\n",
    "    return 'misclassified', np.sum(labels != preds)\n",
    "\n",
    "params['eval_metric'] = []\n",
    "# the argument order is important! if you switch them, you will get error messages\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, feval=misclassified, maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the evaluation results\n",
    "We an get evaluation scores by declaring a dictionary for holding values and passing it as a parameter for ***evals_result*** argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.042831\ttrain-error:0.046522\ttest-misclassified:69\ttrain-misclassified:303\n",
      "[1]\ttest-error:0.039727\ttrain-error:0.043605\ttest-misclassified:64\ttrain-misclassified:284\n",
      "[2]\ttest-error:0.024829\ttrain-error:0.023338\ttest-misclassified:40\ttrain-misclassified:152\n",
      "[3]\ttest-error:0.027312\ttrain-error:0.028251\ttest-misclassified:44\ttrain-misclassified:184\n",
      "[4]\ttest-error:0.011794\ttrain-error:0.011823\ttest-misclassified:19\ttrain-misclassified:77\n"
     ]
    }
   ],
   "source": [
    "evals_results = {}\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, feval=misclassified, maximize=False, evals_result=evals_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': {'error': [0.042831, 0.039727, 0.024829, 0.027312, 0.011794],\n",
      "          'misclassified': [69.0, 64.0, 40.0, 44.0, 19.0]},\n",
      " 'train': {'error': [0.046522, 0.043605, 0.023338, 0.028251, 0.011823],\n",
      "           'misclassified': [303.0, 284.0, 152.0, 184.0, 77.0]}}\n"
     ]
    }
   ],
   "source": [
    "# now reuse these scores for other purposes (such as plotting)\n",
    "pprint(evals_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "There is a nice optimization trick when fitting multiple trees.\n",
    "\n",
    "You can train the model until the validation score stops improving. Validation error needs to decrease at least every early_stopping_rounds to continue training. This approach results in simpler model, because the lowest number of trees will be found (simplicity).\n",
    "\n",
    "In the following example a total number of 1500 trees is to be created, but we are telling it to stop if the validation score does not improve for last ten iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.042831\ttrain-error:0.046522\n",
      "Multiple eval metrics have been passed: 'train-error' will be used for early stopping.\n",
      "\n",
      "Will train until train-error hasn't improved in 10 rounds.\n",
      "[1]\ttest-error:0.039727\ttrain-error:0.043605\n",
      "[2]\ttest-error:0.024829\ttrain-error:0.023338\n",
      "[3]\ttest-error:0.027312\ttrain-error:0.028251\n",
      "[4]\ttest-error:0.011794\ttrain-error:0.011823\n",
      "[5]\ttest-error:0.019863\ttrain-error:0.015968\n",
      "[6]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "[7]\ttest-error:0.013656\ttrain-error:0.010134\n",
      "[8]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "[9]\ttest-error:0\ttrain-error:0.001228\n",
      "[10]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "[11]\ttest-error:0\ttrain-error:0.001228\n",
      "[12]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "[13]\ttest-error:0\ttrain-error:0.001228\n",
      "[14]\ttest-error:0\ttrain-error:0.001228\n",
      "[15]\ttest-error:0\ttrain-error:0.001228\n",
      "[16]\ttest-error:0\ttrain-error:0.001228\n",
      "[17]\ttest-error:0\ttrain-error:0.001228\n",
      "[18]\ttest-error:0\ttrain-error:0.001228\n",
      "[19]\ttest-error:0\ttrain-error:0.001228\n",
      "Stopping. Best iteration:\n",
      "[9]\ttest-error:0\ttrain-error:0.001228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 1500\n",
    "params['eval_metric'] = 'error'\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using early_stopping_rounds parameter, the resulting model will have 3 additional fields - ***bst.best_score***, ***bst.best_iteration*** and ***bst.best_ntree_limit***\n",
    "\n",
    "- Note: train() will return a model from the last iteration, nont the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booster best train score: 0.001228\n",
      "Booster best iteration: 9\n",
      "Booster best number of trees limit: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Booster best train score: {}\".format(bst.best_score))\n",
    "print(\"Booster best iteration: {}\".format(bst.best_iteration))\n",
    "print(\"Booster best number of trees limit: {}\".format(bst.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating results\n",
    "Native XGBoost package provides an option for cross-validating results (but not as sophisticated as sklearn package). \n",
    "\n",
    "The next input shows a basic execution. \n",
    "\n",
    "***Notice that we are passing only single DMatrix, so it would be good to merge train and test into one object to have more training samples***\n",
    "- by default, we get a pandas data frame object (can be changed with as_pandas param)\n",
    "- metrics are passed as an argument (multiple values are allowed)\n",
    "- we can use own evaluation metrics (param feval and maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.010688</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.001187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047312</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023349</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.023349</td>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028264</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.028264</td>\n",
       "      <td>0.000894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011828</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.011828</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0         0.046544        0.010688          0.046544         0.001187\n",
       "1         0.047312        0.010231          0.045161         0.001706\n",
       "2         0.023349        0.006437          0.023349         0.000715\n",
       "3         0.028264        0.008047          0.028264         0.000894\n",
       "4         0.011828        0.003298          0.011828         0.000366\n",
       "5         0.015975        0.004301          0.015975         0.000478\n",
       "6         0.001997        0.001690          0.001997         0.000188\n",
       "7         0.010138        0.002849          0.010138         0.000317\n",
       "8         0.001997        0.001690          0.001997         0.000188\n",
       "9         0.001843        0.001791          0.001843         0.000294"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rounds = 10   # how many estimators\n",
    "\n",
    "hist = xgb.cv(params, dtrain, num_rounds, nfold=10, metrics={'error'}, seed=seed)\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning more...\n",
    "- many parameters are tunable. Each one results in different output. The question is which conbination produces best results.\n",
    "- scikit-learn provides a lot of such modules for us to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Note: from sklearn.cross_validation import StratifiedKFold has been deprecated\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "seed = 342  # fixed seed makes results reproducible\n",
    "np.random.seed(seed)\n",
    "\n",
    "# generate artificial dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=8, n_redundant=3, n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cross-validation strategy for testing. Let's use ***StratifiedKFold*** which guarantees that target labels are equally distributed across each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cv.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "In grid-search, we start by defining a dictionary holding possible parameter values we want to test. \n",
    "- All combinations will be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_grid = { 'max_depth' : [1,2,3], \n",
    "                'n_estimators' : [5, 10, 25, 50],\n",
    "                'learning_rate' : np.linspace(1e-16, 1, 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add a dictionary for fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_fixed = { 'objective' : 'binary:logistic',\n",
    "                 'silent' : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GridSearchCV estimator, We will be looking for combination giving the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_grid = GridSearchCV( estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "                         param_grid=params_grid,\n",
    "                         scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the calculations, notice that we will have 3 \\* 4 \\* 3 \\* 10 = 360 models created to test all combinations.\n",
    "- you should always have rough estimations about what is going to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=342, silent=1, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [1, 2, 3], 'n_estimators': [5, 10, 25, 50], 'learning_rate': array([  1.00000e-16,   5.00000e-01,   1.00000e+00])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at all obtained scores, and try to manually see what matters and what not, A quick glance looks that the larger n_estimators then the accuracy is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.50400, std: 0.00071, params: {'max_depth': 1, 'n_estimators': 5, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 1, 'n_estimators': 10, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 1, 'n_estimators': 25, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 1, 'n_estimators': 50, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 2, 'n_estimators': 5, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 2, 'n_estimators': 10, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 2, 'n_estimators': 25, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 2, 'n_estimators': 50, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 3, 'n_estimators': 5, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 3, 'n_estimators': 10, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 3, 'n_estimators': 25, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.50400, std: 0.00071, params: {'max_depth': 3, 'n_estimators': 50, 'learning_rate': 9.9999999999999998e-17},\n",
       " mean: 0.81300, std: 0.01374, params: {'max_depth': 1, 'n_estimators': 5, 'learning_rate': 0.5},\n",
       " mean: 0.82800, std: 0.01352, params: {'max_depth': 1, 'n_estimators': 10, 'learning_rate': 0.5},\n",
       " mean: 0.81900, std: 0.02670, params: {'max_depth': 1, 'n_estimators': 25, 'learning_rate': 0.5},\n",
       " mean: 0.82800, std: 0.02183, params: {'max_depth': 1, 'n_estimators': 50, 'learning_rate': 0.5},\n",
       " mean: 0.82700, std: 0.01662, params: {'max_depth': 2, 'n_estimators': 5, 'learning_rate': 0.5},\n",
       " mean: 0.83100, std: 0.02403, params: {'max_depth': 2, 'n_estimators': 10, 'learning_rate': 0.5},\n",
       " mean: 0.84400, std: 0.01525, params: {'max_depth': 2, 'n_estimators': 25, 'learning_rate': 0.5},\n",
       " mean: 0.85200, std: 0.01236, params: {'max_depth': 2, 'n_estimators': 50, 'learning_rate': 0.5},\n",
       " mean: 0.83600, std: 0.01781, params: {'max_depth': 3, 'n_estimators': 5, 'learning_rate': 0.5},\n",
       " mean: 0.84100, std: 0.01743, params: {'max_depth': 3, 'n_estimators': 10, 'learning_rate': 0.5},\n",
       " mean: 0.84800, std: 0.01005, params: {'max_depth': 3, 'n_estimators': 25, 'learning_rate': 0.5},\n",
       " mean: 0.85000, std: 0.01343, params: {'max_depth': 3, 'n_estimators': 50, 'learning_rate': 0.5},\n",
       " mean: 0.79100, std: 0.04808, params: {'max_depth': 1, 'n_estimators': 5, 'learning_rate': 1.0},\n",
       " mean: 0.81000, std: 0.02206, params: {'max_depth': 1, 'n_estimators': 10, 'learning_rate': 1.0},\n",
       " mean: 0.82000, std: 0.02544, params: {'max_depth': 1, 'n_estimators': 25, 'learning_rate': 1.0},\n",
       " mean: 0.79900, std: 0.03702, params: {'max_depth': 1, 'n_estimators': 50, 'learning_rate': 1.0},\n",
       " mean: 0.82500, std: 0.01571, params: {'max_depth': 2, 'n_estimators': 5, 'learning_rate': 1.0},\n",
       " mean: 0.84100, std: 0.00642, params: {'max_depth': 2, 'n_estimators': 10, 'learning_rate': 1.0},\n",
       " mean: 0.83400, std: 0.00118, params: {'max_depth': 2, 'n_estimators': 25, 'learning_rate': 1.0},\n",
       " mean: 0.84400, std: 0.01205, params: {'max_depth': 2, 'n_estimators': 50, 'learning_rate': 1.0},\n",
       " mean: 0.82400, std: 0.00518, params: {'max_depth': 3, 'n_estimators': 5, 'learning_rate': 1.0},\n",
       " mean: 0.83200, std: 0.01305, params: {'max_depth': 3, 'n_estimators': 10, 'learning_rate': 1.0},\n",
       " mean: 0.83600, std: 0.01454, params: {'max_depth': 3, 'n_estimators': 25, 'learning_rate': 1.0},\n",
       " mean: 0.84600, std: 0.00765, params: {'max_depth': 3, 'n_estimators': 50, 'learning_rate': 1.0}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.00378195,  0.00530767,  0.0104692 ,  0.01890151,  0.00489839,\n",
       "         0.00806657,  0.01717567,  0.0326732 ,  0.00633375,  0.01096416,\n",
       "         0.02430415,  0.04737369,  0.0035634 ,  0.0053273 ,  0.01067591,\n",
       "         0.01940123,  0.00499018,  0.00802461,  0.01740456,  0.03239552,\n",
       "         0.00647394,  0.01097043,  0.02415196,  0.0455366 ,  0.00352796,\n",
       "         0.00535035,  0.01058149,  0.01943088,  0.00496944,  0.00795603,\n",
       "         0.01733494,  0.03220979,  0.00630943,  0.01085432,  0.02353088,\n",
       "         0.04179748]),\n",
       " 'mean_score_time': array([ 0.0004526 ,  0.000434  ,  0.00045482,  0.00061456,  0.00043837,\n",
       "         0.00042892,  0.00060654,  0.00075706,  0.00044306,  0.00049814,\n",
       "         0.00065096,  0.0007666 ,  0.00042351,  0.00042955,  0.000489  ,\n",
       "         0.00059811,  0.00043559,  0.00046031,  0.00059374,  0.00085084,\n",
       "         0.00043654,  0.0005188 ,  0.00074784,  0.0010922 ,  0.00042987,\n",
       "         0.0004406 ,  0.00049535,  0.00070222,  0.00043988,  0.00045919,\n",
       "         0.00068045,  0.00087516,  0.0004491 ,  0.0005149 ,  0.0007542 ,\n",
       "         0.00106629]),\n",
       " 'mean_test_score': array([ 0.504,  0.504,  0.504,  0.504,  0.504,  0.504,  0.504,  0.504,\n",
       "         0.504,  0.504,  0.504,  0.504,  0.813,  0.828,  0.819,  0.828,\n",
       "         0.827,  0.831,  0.844,  0.852,  0.836,  0.841,  0.848,  0.85 ,\n",
       "         0.791,  0.81 ,  0.82 ,  0.799,  0.825,  0.841,  0.834,  0.844,\n",
       "         0.824,  0.832,  0.836,  0.846]),\n",
       " 'mean_train_score': array([ 0.50400025,  0.50400025,  0.50400025,  0.50400025,  0.50400025,\n",
       "         0.50400025,  0.50400025,  0.50400025,  0.50400025,  0.50400025,\n",
       "         0.50400025,  0.50400025,  0.83649016,  0.85099792,  0.86799918,\n",
       "         0.88799744,  0.85649518,  0.88149919,  0.93699622,  0.97799749,\n",
       "         0.89499394,  0.93849697,  0.98449949,  1.        ,  0.81847665,\n",
       "         0.85250318,  0.88699869,  0.91449921,  0.87700594,  0.91949746,\n",
       "         0.97349598,  1.        ,  0.92249746,  0.97149924,  1.        ,  1.        ]),\n",
       " 'param_learning_rate': masked_array(data = [9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17\n",
       "  9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17\n",
       "  9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17\n",
       "  9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17 0.5\n",
       "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
       "  1.0 1.0 1.0 1.0 1.0],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_max_depth': masked_array(data = [1 1 1 1 2 2 2 2 3 3 3 3 1 1 1 1 2 2 2 2 3 3 3 3 1 1 1 1 2 2 2 2 3 3 3 3],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_estimators': masked_array(data = [5 10 25 50 5 10 25 50 5 10 25 50 5 10 25 50 5 10 25 50 5 10 25 50 5 10 25\n",
       "  50 5 10 25 50 5 10 25 50],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 5},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 10},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 25},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 50},\n",
       "  {'learning_rate': 9.9999999999999998e-17, 'max_depth': 2, 'n_estimators': 5},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 10},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 25},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 50},\n",
       "  {'learning_rate': 9.9999999999999998e-17, 'max_depth': 3, 'n_estimators': 5},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 25},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 5},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 25},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 50},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 5},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 25},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 50},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 5},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 25},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 5},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 10},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 25},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 50},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 5},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 10},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 25},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 50},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 5},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 25},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50}),\n",
       " 'rank_test_score': array([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 21, 14, 20, 14, 16,\n",
       "        13,  5,  1,  9,  7,  3,  2, 24, 22, 19, 23, 17,  7, 11,  5, 18, 12,\n",
       "         9,  4], dtype=int32),\n",
       " 'split0_test_score': array([ 0.50299401,  0.50299401,  0.50299401,  0.50299401,  0.50299401,\n",
       "         0.50299401,  0.50299401,  0.50299401,  0.50299401,  0.50299401,\n",
       "         0.50299401,  0.50299401,  0.7994012 ,  0.82934132,  0.79341317,\n",
       "         0.81137725,  0.82035928,  0.81137725,  0.83832335,  0.83532934,\n",
       "         0.81137725,  0.81736527,  0.83832335,  0.83233533,  0.72754491,\n",
       "         0.79041916,  0.7994012 ,  0.78443114,  0.80538922,  0.83832335,\n",
       "         0.83233533,  0.82934132,  0.82634731,  0.83832335,  0.81736527,\n",
       "         0.83532934]),\n",
       " 'split0_train_score': array([ 0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,\n",
       "         0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,\n",
       "         0.5045045 ,  0.5045045 ,  0.81681682,  0.84684685,  0.86636637,\n",
       "         0.88288288,  0.84684685,  0.87987988,  0.92942943,  0.97297297,\n",
       "         0.88288288,  0.93243243,  0.98348348,  1.        ,  0.77177177,\n",
       "         0.85885886,  0.88438438,  0.91291291,  0.88888889,  0.91441441,\n",
       "         0.96546547,  1.        ,  0.91741742,  0.96996997,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,\n",
       "         0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,\n",
       "         0.5045045 ,  0.5045045 ,  0.80780781,  0.81081081,  0.80780781,\n",
       "         0.81381381,  0.81081081,  0.81681682,  0.82882883,  0.85585586,\n",
       "         0.84384384,  0.84684685,  0.84384384,  0.85285285,  0.8018018 ,\n",
       "         0.7987988 ,  0.8048048 ,  0.76276276,  0.82582583,  0.83483483,\n",
       "         0.83483483,  0.85885886,  0.81681682,  0.81381381,  0.83783784,\n",
       "         0.84984985]),\n",
       " 'split1_train_score': array([ 0.50374813,  0.50374813,  0.50374813,  0.50374813,  0.50374813,\n",
       "         0.50374813,  0.50374813,  0.50374813,  0.50374813,  0.50374813,\n",
       "         0.50374813,  0.50374813,  0.85307346,  0.85907046,  0.87106447,\n",
       "         0.89655172,  0.86506747,  0.89355322,  0.94602699,  0.98350825,\n",
       "         0.90704648,  0.94902549,  0.99250375,  1.        ,  0.84407796,\n",
       "         0.85007496,  0.88455772,  0.91754123,  0.88305847,  0.92803598,\n",
       "         0.97301349,  1.        ,  0.93103448,  0.97901049,  1.        ,  1.        ]),\n",
       " 'split2_test_score': array([ 0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,\n",
       "         0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,  0.5045045 ,\n",
       "         0.5045045 ,  0.5045045 ,  0.83183183,  0.84384384,  0.85585586,\n",
       "         0.85885886,  0.84984985,  0.86486486,  0.86486486,  0.86486486,\n",
       "         0.85285285,  0.85885886,  0.86186186,  0.86486486,  0.84384384,\n",
       "         0.84084084,  0.85585586,  0.84984985,  0.84384384,  0.84984985,\n",
       "         0.83483483,  0.84384384,  0.82882883,  0.84384384,  0.85285285,\n",
       "         0.85285285]),\n",
       " 'split2_train_score': array([ 0.50374813,  0.50374813,  0.50374813,  0.50374813,  0.50374813,\n",
       "         0.50374813,  0.50374813,  0.50374813,  0.50374813,  0.50374813,\n",
       "         0.50374813,  0.50374813,  0.83958021,  0.84707646,  0.86656672,\n",
       "         0.88455772,  0.85757121,  0.87106447,  0.93553223,  0.97751124,\n",
       "         0.89505247,  0.93403298,  0.97751124,  1.        ,  0.83958021,\n",
       "         0.84857571,  0.89205397,  0.91304348,  0.85907046,  0.91604198,\n",
       "         0.982009  ,  1.        ,  0.91904048,  0.96551724,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([  1.77814268e-04,   2.98413974e-05,   6.43582975e-06,\n",
       "          1.73616570e-04,   9.76374458e-05,   1.06912650e-04,\n",
       "          1.87348262e-04,   4.08182696e-05,   5.59765439e-05,\n",
       "          1.03527486e-04,   3.73242134e-04,   3.44678200e-04,\n",
       "          2.07145365e-05,   4.14310548e-05,   5.19996697e-05,\n",
       "          8.96067683e-05,   5.80522993e-05,   1.66513640e-04,\n",
       "          3.42350041e-05,   3.00655586e-04,   1.16695334e-04,\n",
       "          4.94641786e-05,   2.67980315e-05,   2.64566881e-04,\n",
       "          1.59275190e-05,   3.21272906e-05,   5.58473151e-05,\n",
       "          1.92739365e-04,   7.61607949e-05,   3.71219080e-05,\n",
       "          3.92910202e-04,   1.66586678e-04,   6.39970222e-05,\n",
       "          8.23163139e-05,   1.43975652e-04,   6.22762665e-04]),\n",
       " 'std_score_time': array([  3.33742487e-05,   5.78898552e-06,   1.97607951e-05,\n",
       "          5.20205568e-05,   3.31109152e-05,   1.23794126e-05,\n",
       "          5.43806944e-05,   1.74478677e-06,   1.28544740e-05,\n",
       "          1.83125888e-05,   7.49540725e-05,   3.01743702e-05,\n",
       "          1.19532042e-05,   1.61355350e-05,   9.33391736e-06,\n",
       "          9.45359648e-06,   1.36378762e-05,   3.25935628e-05,\n",
       "          9.47761751e-06,   7.87321381e-05,   1.66324373e-06,\n",
       "          4.99261636e-05,   1.58834418e-05,   1.96922778e-05,\n",
       "          1.78713240e-05,   4.16152556e-06,   2.20106825e-05,\n",
       "          2.35035524e-05,   7.85214376e-06,   2.25688929e-05,\n",
       "          4.51096521e-05,   2.10433568e-05,   4.60805544e-06,\n",
       "          2.45191482e-05,   2.70233434e-05,   2.20364927e-05]),\n",
       " 'std_test_score': array([ 0.00071241,  0.00071241,  0.00071241,  0.00071241,  0.00071241,\n",
       "         0.00071241,  0.00071241,  0.00071241,  0.00071241,  0.00071241,\n",
       "         0.00071241,  0.00071241,  0.01374181,  0.01351236,  0.02669663,\n",
       "         0.02182683,  0.01660932,  0.024031  ,  0.01524391,  0.01236472,\n",
       "         0.01782032,  0.01744027,  0.01005054,  0.01343551,  0.04810005,\n",
       "         0.02205844,  0.02543082,  0.0370028 ,  0.01571378,  0.00641331,\n",
       "         0.00117887,  0.01205399,  0.00517564,  0.01304617,  0.01454942,\n",
       "         0.00765532]),\n",
       " 'std_train_score': array([ 0.00035656,  0.00035656,  0.00035656,  0.00035656,  0.00035656,\n",
       "         0.00035656,  0.00035656,  0.00035656,  0.00035656,  0.00035656,\n",
       "         0.00035656,  0.00035656,  0.01496212,  0.00570892,  0.00216903,\n",
       "         0.00608731,  0.00747735,  0.00925212,  0.00685454,  0.00431473,\n",
       "         0.00986483,  0.00747341,  0.00616268,  0.        ,  0.03307634,\n",
       "         0.00453563,  0.00357532,  0.0021517 ,  0.01290373,  0.0060741 ,\n",
       "         0.00676248,  0.        ,  0.00607284,  0.00561373,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are too many results, we can filter them manually to get the best combination\n",
    "- Note: looking for best parameters is an iterative process. You should start with coarsed-granularity and move to more detailed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy obtained: 0.852\n",
      "Parameters\n",
      "\tmax_depth: 2\n",
      "\tn_estimators: 50\n",
      "\tlearning_rate: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Best accuracy obtained: {0}\".format(bst_grid.best_score_))\n",
    "print(\"Parameters\")\n",
    "for key, value in bst_grid.best_params_.items():\n",
    "    print(\"\\t{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Grid-Search\n",
    "when the number of parameters and their values is getting big, the traditional grid-search approach quickly becomes ineffective.\n",
    "- A possible solution might be to randomly pick certain parameters from their distribution. While it's not an exhaustive solution, it's worth giving a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a parameters distribution dictionary:\n",
    "params_dist_grid = { 'max_depth' : [1, 2, 3, 4],\n",
    "                     'gamma' : [0, 0.5, 1],\n",
    "                     'n_estimators' : randint(1, 1001),   # uniform discrete random distribution\n",
    "                     'learning_rate' : uniform,           # gaussain distribution\n",
    "                     'subsample' : uniform(),             # gaussain distribution\n",
    "                     #'colsample_bytree' : uniform()       # gaussain distribution\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ***RandomizedSearchCV*** to randomly pick 10 combinations of parameters. \n",
    "- with this approach you can easily control the number of tested models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=342, shuffle=True),\n",
       "          error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=342, silent=1, subsample=1),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f89e35c04a8>, 'max_depth': [1, 2, 3, 4], 'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f89e35c0518>, 'gamma': [0, 0.5, 1], 'learning_rate': <scipy.stats._continuous_distns.uniform_gen object at 0x7f89ec250470>},\n",
       "          pre_dispatch='2*n_jobs', random_state=342, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid = RandomizedSearchCV( estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "                              param_distributions=params_dist_grid,\n",
    "                              n_iter=10,\n",
    "                              scoring='accuracy',\n",
    "                              cv = cv,\n",
    "                              random_state=seed)\n",
    "\n",
    "# Train the classifier\n",
    "rs_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some handy properties allowing to quickly analyze best estimator, parameters and obtained score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=1, learning_rate=0.25244657272498761, max_delta_step=0,\n",
       "       max_depth=2, min_child_weight=1, missing=None, n_estimators=541,\n",
       "       nthread=-1, objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=342, silent=1,\n",
       "       subsample=0.47402131267540315)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 1,\n",
       " 'learning_rate': 0.25244657272498761,\n",
       " 'max_depth': 2,\n",
       " 'n_estimators': 541,\n",
       " 'subsample': 0.47402131267540315}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86299999999999999"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
