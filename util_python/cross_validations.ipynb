{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation is an important part of machine learning. There are different ways for cross validation. \n",
    "We can use it to evaluate:\n",
    "- what models are more effective\n",
    "- what parameters to use for a specific model\n",
    "- selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# note: sklearn.cross_validation import train_test_split will be deprecated\n",
    "# note: sklearn.cross_validation import cross_val_score will be deprecated\n",
    "# note: sklearn.cross_validation import KFold will be deprecated\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import math, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation Metrics\n",
    "In order to evaluate each model system, we need to have metrics systems to help us. \n",
    "- for classification: the target(s) are category data, so we use ***metrics.accuracy_score*** for measuring\n",
    "  * **error** - binary classification error rate. It is calculated as # (wrong cases) / #(all casees). Treat predicted values with probability p > 0.5 as positive\n",
    "  * **merror** - multiclass classification error rate. It is calculated as # (wrong cases) / #(all casees).\n",
    "- for regression: the target(s) are continuous data. The goal is to ___minimize___ them in the loss functions:\n",
    "  * **Mean Absolute Error (MAE): metrics.mean_absolute_error** \n",
    "  $$mae = \\frac{1}{n}\\sum_{i=0}^n|y_{i} - \\bar{y}_{i}|$$\n",
    "  * **Mean Square Error (MSE): metics.mean_squared_error **\n",
    "  $$mse = \\frac{1}{n}\\sum_{i=0}^n(y_{i} - \\bar{y}_{i})^2$$\n",
    "  * **Root Mean Square Error (RMSE) **\n",
    "  $$rmse = \\sqrt{\\frac{1}{n}\\sum_{i=0}^n(y_{i} - \\bar{y}_{i})^2}$$\n",
    "  * **Logloss** - negaive log-likelihood \n",
    "  * **AUC**  - area under curve          (***Maximize this***)\n",
    "  * **NDCG** - normalized discounted cumulative gain   (***Maximize this***)\n",
    "  * **MAP**  - mean average precision                  (***Maximize this***)\n",
    "- by default, an error metric will be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without cross_validation\n",
    "- run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model score is: 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "y = load_iris().target\n",
    "\n",
    "# preprocessing data and split it into train and test sets\n",
    "X = preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# model\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The model score is:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do train_test_split, part of the samples are used for testing. However, it provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy!\n",
    "\n",
    "How to make use of those test data for training ===> K-folds cross_validation would solve this problem:\n",
    "- A model is trained using k-1 of the folds as training data;\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "- The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop (using different test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.2581988897471611, 0.0, 0.3651483716701107, 0.2581988897471611, 0.0, 0.0, 0.0, 0.2581988897471611, 0.2581988897471611]\n",
      "0.373890883903\n"
     ]
    }
   ],
   "source": [
    "# Here is the step by step cross validation (cv)\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "# cv fold\n",
    "nfolds = 10\n",
    "kf = KFold(n_splits=nfolds, shuffle=True, random_state=int(time.time()))\n",
    "\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "rmse = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"%s, %s\" % (train_index, test_index))\n",
    "    #print(\"%d, %d\" % (len(train_index), len(test_index)))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    rmse.append(math.sqrt(metrics.mean_squared_error(y_pred, y_test)))\n",
    "\n",
    "print(rmse)\n",
    "print(np.sqrt(np.mean(rmse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Good** thing is that you usually don't need to inplement the details about cross validation. The sklearn package provides a high level function ***cross_val_score()*** to do all the above.\n",
    "- In addition, for classification problems, ***stratified sampling*** is recommended for creating the folds; that is\n",
    "  * each response (or target) should be represented with equal proportions in each of the K folds.\n",
    "  * **sklearn.cross_val_score()** function does this by default!\n",
    "- Validation options are:\n",
    "    - ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.93333333  1.          0.93333333  1.          0.93333333\n",
      "  0.86666667  1.          1.          1.        ]\n",
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "# Here is the simplified version of cross validation (cv)\n",
    "\n",
    "X = load_iris().data\n",
    "X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "#scores = -cross_val_score(svm.SVC(), X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "scores = cross_val_score(SVC(), X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use this to tune model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95999999999999996, 0.95333333333333337, 0.96666666666666656, 0.96666666666666656, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.98000000000000009, 0.96666666666666656, 0.96666666666666656, 0.97333333333333338, 0.95999999999999996, 0.96666666666666656, 0.95999999999999996, 0.96666666666666656, 0.95333333333333337, 0.95333333333333337, 0.95333333333333337]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f47a78d4128>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFkCAYAAACJu/k0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47a80390b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "#X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "k_range = range(1,31)\n",
    "k_scores = []\n",
    "\n",
    "for i in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    k_scores.append(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n",
    "    \n",
    "print(k_scores)\n",
    "\n",
    "# plotting\n",
    "plt.plot(k_range, k_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The following will be done in XGBoost\n",
    "- the dataset will be taken from Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.042831\ttrain-error:0.046522\n",
      "[1]\teval-error:0.039727\ttrain-error:0.043605\n",
      "[2]\teval-error:0.024829\ttrain-error:0.023338\n",
      "[3]\teval-error:0.027312\ttrain-error:0.028251\n",
      "[4]\teval-error:0.011794\ttrain-error:0.011823\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from pprint import pprint\n",
    "\n",
    "# for reproducibility, if you don't want this, you could use time.time() to get different value every time\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "dtrain = xgb.DMatrix('../../data/agaricus.txt.train')\n",
    "dtest  = xgb.DMatrix('../../data/agaricus.txt.test')\n",
    "\n",
    "# train parameters - we are going to use 5 decision tree stumps with average learning rate.\n",
    "# the defaul error metric is 'error'\n",
    "params = {'objective' : 'binary:logistic',\n",
    "          'max_depth' : 2,\n",
    "          'silent' : 1,\n",
    "          'eta' : 0.5}\n",
    "num_rounds = 5\n",
    "watch_list = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "# training\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's change the error metric to logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.377093\ttrain-logloss:0.380598\n",
      "[1]\teval-logloss:0.245811\ttrain-logloss:0.247331\n",
      "[2]\teval-logloss:0.174941\ttrain-logloss:0.175047\n",
      "[3]\teval-logloss:0.124244\ttrain-logloss:0.122301\n",
      "[4]\teval-logloss:0.089699\ttrain-logloss:0.089889\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = 'logloss'\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we could use multiple error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.960373\teval-map:0.98915\ttrain-auc:0.958228\ttrain-map:0.966873\n",
      "[1]\teval-auc:0.992342\teval-map:0.991732\ttrain-auc:0.99115\ttrain-map:0.989131\n",
      "[2]\teval-auc:0.996324\teval-map:0.995753\ttrain-auc:0.995134\ttrain-map:0.995014\n",
      "[3]\teval-auc:0.998298\teval-map:0.998303\ttrain-auc:0.99819\ttrain-map:0.998176\n",
      "[4]\teval-auc:0.999339\teval-map:0.999292\ttrain-auc:0.999085\ttrain-map:0.999081\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = ['auc', 'map']\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom evaluation metric\n",
    "In order to create our own evaluation metric, the only thing needed to do is to create a method taking two arguments - ***predicted probabilities*** and ***DMatrix*** objet holding training data\n",
    "\n",
    "In this example, our classification metric will simply count the number of mis-classified examples assuming that classes with p > 0.5 are positive. You can change this threshold if you want more certainty\n",
    "\n",
    "The algorithm is getting better when the number of mis-classified examples is getting lower. Remember to also set the argument ***maximize=False*** while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.042831\ttrain-error:0.046522\teval-misclassified:69\ttrain-misclassified:303\n",
      "[1]\teval-error:0.039727\ttrain-error:0.043605\teval-misclassified:64\ttrain-misclassified:284\n",
      "[2]\teval-error:0.024829\ttrain-error:0.023338\teval-misclassified:40\ttrain-misclassified:152\n",
      "[3]\teval-error:0.027312\ttrain-error:0.028251\teval-misclassified:44\ttrain-misclassified:184\n",
      "[4]\teval-error:0.011794\ttrain-error:0.011823\teval-misclassified:19\ttrain-misclassified:77\n"
     ]
    }
   ],
   "source": [
    "def misclassified(pred_probs, dtrain):\n",
    "    labels = dtrain.get_label()   # obtain true labels\n",
    "    preds  = pred_probs > 0.5     # obtain predicted values\n",
    "    return 'misclassified', np.sum(labels != preds)\n",
    "\n",
    "params['eval_metric'] = []\n",
    "# the argument order is important! if you switch them, you will get error messages\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, feval=misclassified, maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the evaluation results\n",
    "We can get evaluation scores by declaring a dictionary for holding values and passing it as a parameter for ***evals_result*** argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.042831\ttrain-error:0.046522\teval-misclassified:69\ttrain-misclassified:303\n",
      "[1]\teval-error:0.039727\ttrain-error:0.043605\teval-misclassified:64\ttrain-misclassified:284\n",
      "[2]\teval-error:0.024829\ttrain-error:0.023338\teval-misclassified:40\ttrain-misclassified:152\n",
      "[3]\teval-error:0.027312\ttrain-error:0.028251\teval-misclassified:44\ttrain-misclassified:184\n",
      "[4]\teval-error:0.011794\ttrain-error:0.011823\teval-misclassified:19\ttrain-misclassified:77\n"
     ]
    }
   ],
   "source": [
    "evals_results = {}\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, feval=misclassified, maximize=False, evals_result=evals_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval': {'error': [0.042831, 0.039727, 0.024829, 0.027312, 0.011794],\n",
      "          'misclassified': [69.0, 64.0, 40.0, 44.0, 19.0]},\n",
      " 'train': {'error': [0.046522, 0.043605, 0.023338, 0.028251, 0.011823],\n",
      "           'misclassified': [303.0, 284.0, 152.0, 184.0, 77.0]}}\n"
     ]
    }
   ],
   "source": [
    "# now reuse these scores for other purposes (such as plotting)\n",
    "pprint(evals_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "There is a nice optimization trick when fitting multiple trees.\n",
    "\n",
    "You can train the model until the validation score stops improving. Validation error needs to decrease at least every early_stopping_rounds to continue training. This approach results in simpler model, because the lowest number of trees will be found (simplicity).\n",
    "\n",
    "In the following example a total number of 1500 trees is to be created, but we are telling it to stop if the validation score does not improve for last ten iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.042831\ttrain-error:0.046522\n",
      "Multiple eval metrics have been passed: 'train-error' will be used for early stopping.\n",
      "\n",
      "Will train until train-error hasn't improved in 10 rounds.\n",
      "[1]\teval-error:0.039727\ttrain-error:0.043605\n",
      "[2]\teval-error:0.024829\ttrain-error:0.023338\n",
      "[3]\teval-error:0.027312\ttrain-error:0.028251\n",
      "[4]\teval-error:0.011794\ttrain-error:0.011823\n",
      "[5]\teval-error:0.019863\ttrain-error:0.015968\n",
      "[6]\teval-error:0.001862\ttrain-error:0.001996\n",
      "[7]\teval-error:0.013656\ttrain-error:0.010134\n",
      "[8]\teval-error:0.001862\ttrain-error:0.001996\n",
      "[9]\teval-error:0\ttrain-error:0.001228\n",
      "[10]\teval-error:0.001862\ttrain-error:0.001996\n",
      "[11]\teval-error:0\ttrain-error:0.001228\n",
      "[12]\teval-error:0.001862\ttrain-error:0.001996\n",
      "[13]\teval-error:0\ttrain-error:0.001228\n",
      "[14]\teval-error:0\ttrain-error:0.001228\n",
      "[15]\teval-error:0\ttrain-error:0.001228\n",
      "[16]\teval-error:0\ttrain-error:0.001228\n",
      "[17]\teval-error:0\ttrain-error:0.001228\n",
      "[18]\teval-error:0\ttrain-error:0.001228\n",
      "[19]\teval-error:0\ttrain-error:0.001228\n",
      "Stopping. Best iteration:\n",
      "[9]\teval-error:0\ttrain-error:0.001228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 1500\n",
    "params['eval_metric'] = 'error'\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using early_stopping_rounds parameter, the resulting model will have 3 additional fields - ***bst.best_score***, ***bst.best_iteration*** and ***bst.best_ntree_limit***\n",
    "\n",
    "- Note: train() will return a model from the last iteration, not the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booster best train score: 0.001228\n",
      "Booster best iteration: 9\n",
      "Booster best number of trees limit: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Booster best train score: {}\".format(bst.best_score))\n",
    "print(\"Booster best iteration: {}\".format(bst.best_iteration))\n",
    "print(\"Booster best number of trees limit: {}\".format(bst.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating results\n",
    "Native XGBoost package provides an option for cross-validating results (but not as sophisticated as sklearn package). \n",
    "\n",
    "The next input shows a basic execution. \n",
    "\n",
    "***Notice that we are passing only single DMatrix, so it would be good to merge train and test into one object to have more training samples***\n",
    "- by default, we get a pandas data frame object (can be changed with as_pandas param)\n",
    "- metrics are passed as an argument (multiple values are allowed)\n",
    "- we can use own evaluation metrics (param feval and maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.010688</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.001187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047312</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023349</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.023349</td>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028264</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.028264</td>\n",
       "      <td>0.000894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011828</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.011828</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0         0.046544        0.010688          0.046544         0.001187\n",
       "1         0.047312        0.010231          0.045161         0.001706\n",
       "2         0.023349        0.006437          0.023349         0.000715\n",
       "3         0.028264        0.008047          0.028264         0.000894\n",
       "4         0.011828        0.003298          0.011828         0.000366\n",
       "5         0.015975        0.004301          0.015975         0.000478\n",
       "6         0.001997        0.001690          0.001997         0.000188\n",
       "7         0.010138        0.002849          0.010138         0.000317\n",
       "8         0.001997        0.001690          0.001997         0.000188\n",
       "9         0.001843        0.001791          0.001843         0.000294"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rounds = 10   # how many estimators\n",
    "\n",
    "hist = xgb.cv(params, dtrain, num_rounds, nfold=10, metrics={'error'}, seed=seed)\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning more\n",
    "- many parameters are tunable. Each one results in different output. The question is which conbination produces best results.\n",
    "- scikit-learn provides a lot of such modules for us to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Note: from sklearn.cross_validation import StratifiedKFold has been deprecated\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "seed = 342  # fixed seed makes results reproducible\n",
    "np.random.seed(seed)\n",
    "\n",
    "# generate artificial dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=8, n_redundant=3, n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cross-validation strategy for testing. Let's use ***StratifiedKFold*** which guarantees that target labels are equally distributed across each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cv.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "In grid-search, we start by defining a dictionary holding possible parameter values we want to test. \n",
    "- All combinations will be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_grid = { 'max_depth' : [1,2,3], \n",
    "                'n_estimators' : [5, 10, 25, 50],\n",
    "                'learning_rate' : np.linspace(1e-16, 1, 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add a dictionary for fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_fixed = { 'objective' : 'binary:logistic',\n",
    "                 'silent' : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GridSearchCV estimator, We will be looking for combination giving the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_grid = GridSearchCV( estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "                         param_grid=params_grid,\n",
    "                         cv=cv,\n",
    "                         scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the calculations, notice that we will have 3 \\* 4 \\* 3 \\* 10 = 360 models created to test all combinations.\n",
    "- you should always have rough estimations about what is going to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=342, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=342, silent=1, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': array([  1.00000e-16,   5.00000e-01,   1.00000e+00]), 'n_estimators': [5, 10, 25, 50], 'max_depth': [1, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at all obtained scores, and try to manually see what matters and what not, A quick glance looks that the larger n_estimators then the accuracy is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 5, 'max_depth': 1},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 10, 'max_depth': 1},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 25, 'max_depth': 1},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 50, 'max_depth': 1},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 5, 'max_depth': 2},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 10, 'max_depth': 2},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 25, 'max_depth': 2},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 50, 'max_depth': 2},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 5, 'max_depth': 3},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 10, 'max_depth': 3},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 25, 'max_depth': 3},\n",
       " mean: 0.50400, std: 0.00200, params: {'learning_rate': 9.9999999999999998e-17, 'n_estimators': 50, 'max_depth': 3},\n",
       " mean: 0.79900, std: 0.06194, params: {'learning_rate': 0.5, 'n_estimators': 5, 'max_depth': 1},\n",
       " mean: 0.82200, std: 0.05424, params: {'learning_rate': 0.5, 'n_estimators': 10, 'max_depth': 1},\n",
       " mean: 0.82400, std: 0.04510, params: {'learning_rate': 0.5, 'n_estimators': 25, 'max_depth': 1},\n",
       " mean: 0.84000, std: 0.04030, params: {'learning_rate': 0.5, 'n_estimators': 50, 'max_depth': 1},\n",
       " mean: 0.82900, std: 0.03537, params: {'learning_rate': 0.5, 'n_estimators': 5, 'max_depth': 2},\n",
       " mean: 0.83800, std: 0.03832, params: {'learning_rate': 0.5, 'n_estimators': 10, 'max_depth': 2},\n",
       " mean: 0.85600, std: 0.03486, params: {'learning_rate': 0.5, 'n_estimators': 25, 'max_depth': 2},\n",
       " mean: 0.85300, std: 0.03812, params: {'learning_rate': 0.5, 'n_estimators': 50, 'max_depth': 2},\n",
       " mean: 0.84300, std: 0.03213, params: {'learning_rate': 0.5, 'n_estimators': 5, 'max_depth': 3},\n",
       " mean: 0.85600, std: 0.03420, params: {'learning_rate': 0.5, 'n_estimators': 10, 'max_depth': 3},\n",
       " mean: 0.86600, std: 0.03383, params: {'learning_rate': 0.5, 'n_estimators': 25, 'max_depth': 3},\n",
       " mean: 0.87200, std: 0.02623, params: {'learning_rate': 0.5, 'n_estimators': 50, 'max_depth': 3},\n",
       " mean: 0.80800, std: 0.04202, params: {'learning_rate': 1.0, 'n_estimators': 5, 'max_depth': 1},\n",
       " mean: 0.81400, std: 0.03422, params: {'learning_rate': 1.0, 'n_estimators': 10, 'max_depth': 1},\n",
       " mean: 0.81900, std: 0.02360, params: {'learning_rate': 1.0, 'n_estimators': 25, 'max_depth': 1},\n",
       " mean: 0.82000, std: 0.03478, params: {'learning_rate': 1.0, 'n_estimators': 50, 'max_depth': 1},\n",
       " mean: 0.83000, std: 0.03512, params: {'learning_rate': 1.0, 'n_estimators': 5, 'max_depth': 2},\n",
       " mean: 0.83200, std: 0.03657, params: {'learning_rate': 1.0, 'n_estimators': 10, 'max_depth': 2},\n",
       " mean: 0.84700, std: 0.03961, params: {'learning_rate': 1.0, 'n_estimators': 25, 'max_depth': 2},\n",
       " mean: 0.84700, std: 0.04523, params: {'learning_rate': 1.0, 'n_estimators': 50, 'max_depth': 2},\n",
       " mean: 0.84300, std: 0.03001, params: {'learning_rate': 1.0, 'n_estimators': 5, 'max_depth': 3},\n",
       " mean: 0.83900, std: 0.02855, params: {'learning_rate': 1.0, 'n_estimators': 10, 'max_depth': 3},\n",
       " mean: 0.84800, std: 0.03108, params: {'learning_rate': 1.0, 'n_estimators': 25, 'max_depth': 3},\n",
       " mean: 0.86300, std: 0.02977, params: {'learning_rate': 1.0, 'n_estimators': 50, 'max_depth': 3}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.00446646,  0.00655363,  0.01313214,  0.02412491,  0.00617661,\n",
       "         0.00992792,  0.02188349,  0.0414077 ,  0.00786338,  0.01367953,\n",
       "         0.03103051,  0.05946367,  0.00432563,  0.00665994,  0.01350579,\n",
       "         0.02450576,  0.00612512,  0.01017241,  0.02198834,  0.04201226,\n",
       "         0.00807309,  0.01389258,  0.03112109,  0.05917735,  0.00442333,\n",
       "         0.00683696,  0.01383007,  0.02515924,  0.00625587,  0.01035583,\n",
       "         0.02225537,  0.04163256,  0.00801852,  0.01381564,  0.03044968,\n",
       "         0.05619001]),\n",
       " 'mean_score_time': array([ 0.00036659,  0.00036576,  0.00040791,  0.00049007,  0.00037279,\n",
       "         0.00038493,  0.00050042,  0.00049839,  0.00042319,  0.0004364 ,\n",
       "         0.00043237,  0.00050459,  0.00034852,  0.00036247,  0.00040078,\n",
       "         0.00042958,  0.00034518,  0.00036237,  0.00043192,  0.00061283,\n",
       "         0.00037129,  0.00042477,  0.00055037,  0.00065746,  0.0003479 ,\n",
       "         0.00035708,  0.00041983,  0.0004303 ,  0.0003547 ,  0.00036988,\n",
       "         0.00040224,  0.00048759,  0.00034969,  0.00037146,  0.00044339,\n",
       "         0.00059025]),\n",
       " 'mean_test_score': array([ 0.504,  0.504,  0.504,  0.504,  0.504,  0.504,  0.504,  0.504,\n",
       "         0.504,  0.504,  0.504,  0.504,  0.799,  0.822,  0.824,  0.84 ,\n",
       "         0.829,  0.838,  0.856,  0.853,  0.843,  0.856,  0.866,  0.872,\n",
       "         0.808,  0.814,  0.819,  0.82 ,  0.83 ,  0.832,  0.847,  0.847,\n",
       "         0.843,  0.839,  0.848,  0.863]),\n",
       " 'mean_train_score': array([ 0.504     ,  0.504     ,  0.504     ,  0.504     ,  0.504     ,\n",
       "         0.504     ,  0.504     ,  0.504     ,  0.504     ,  0.504     ,\n",
       "         0.504     ,  0.504     ,  0.82599699,  0.84300133,  0.85789122,\n",
       "         0.88044309,  0.84944602,  0.87288592,  0.9238877 ,  0.96455651,\n",
       "         0.89099816,  0.91633189,  0.97466664,  0.99933346,  0.81821438,\n",
       "         0.84866417,  0.87555383,  0.89833249,  0.87167086,  0.90477818,\n",
       "         0.95955613,  0.99622333,  0.90844522,  0.95289304,  0.99888864,  1.        ]),\n",
       " 'param_learning_rate': masked_array(data = [9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17\n",
       "  9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17\n",
       "  9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17\n",
       "  9.9999999999999998e-17 9.9999999999999998e-17 9.9999999999999998e-17 0.5\n",
       "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
       "  1.0 1.0 1.0 1.0 1.0],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_max_depth': masked_array(data = [1 1 1 1 2 2 2 2 3 3 3 3 1 1 1 1 2 2 2 2 3 3 3 3 1 1 1 1 2 2 2 2 3 3 3 3],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_estimators': masked_array(data = [5 10 25 50 5 10 25 50 5 10 25 50 5 10 25 50 5 10 25 50 5 10 25 50 5 10 25\n",
       "  50 5 10 25 50 5 10 25 50],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 5},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 10},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 25},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 1,\n",
       "   'n_estimators': 50},\n",
       "  {'learning_rate': 9.9999999999999998e-17, 'max_depth': 2, 'n_estimators': 5},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 10},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 25},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 50},\n",
       "  {'learning_rate': 9.9999999999999998e-17, 'max_depth': 3, 'n_estimators': 5},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 25},\n",
       "  {'learning_rate': 9.9999999999999998e-17,\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 5},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 25},\n",
       "  {'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 50},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 5},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 25},\n",
       "  {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 50},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 5},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 25},\n",
       "  {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 50},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 5},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 10},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 25},\n",
       "  {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 50},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 5},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 10},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 25},\n",
       "  {'learning_rate': 1.0, 'max_depth': 2, 'n_estimators': 50},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 5},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 25},\n",
       "  {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50}),\n",
       " 'rank_test_score': array([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 24, 19, 18, 12, 17,\n",
       "        14,  4,  6, 10,  4,  2,  1, 23, 22, 21, 20, 16, 15,  8,  8, 10, 13,\n",
       "         7,  3], dtype=int32),\n",
       " 'split0_test_score': array([ 0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.66336634,  0.79207921,  0.77227723,\n",
       "         0.79207921,  0.83168317,  0.81188119,  0.84158416,  0.86138614,\n",
       "         0.82178218,  0.83168317,  0.86138614,  0.88118812,  0.79207921,\n",
       "         0.81188119,  0.83168317,  0.81188119,  0.83168317,  0.83168317,\n",
       "         0.86138614,  0.88118812,  0.83168317,  0.85148515,  0.83168317,\n",
       "         0.86138614]),\n",
       " 'split0_train_score': array([ 0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.77864294,  0.84983315,  0.87096774,\n",
       "         0.88320356,  0.8487208 ,  0.86540601,  0.91879867,  0.96329255,\n",
       "         0.89543938,  0.91212458,  0.96996663,  1.        ,  0.8320356 ,\n",
       "         0.84983315,  0.87319244,  0.89877642,  0.88876529,  0.90878754,\n",
       "         0.95550612,  0.99777531,  0.91546162,  0.96551724,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.86138614,  0.88118812,  0.88118812,\n",
       "         0.9009901 ,  0.88118812,  0.9009901 ,  0.88118812,  0.86138614,\n",
       "         0.87128713,  0.89108911,  0.91089109,  0.88118812,  0.84158416,\n",
       "         0.83168317,  0.86138614,  0.88118812,  0.9009901 ,  0.88118812,\n",
       "         0.92079208,  0.91089109,  0.9009901 ,  0.86138614,  0.84158416,\n",
       "         0.84158416]),\n",
       " 'split1_train_score': array([ 0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.83426029,  0.83870968,  0.85205784,\n",
       "         0.87652948,  0.84760845,  0.87430478,  0.93103448,  0.95995551,\n",
       "         0.87986652,  0.9054505 ,  0.97330367,  1.        ,  0.82091212,\n",
       "         0.84204672,  0.87319244,  0.90211346,  0.8676307 ,  0.89098999,\n",
       "         0.95773081,  0.99555061,  0.89988877,  0.94771969,  0.99888765,  1.        ]),\n",
       " 'split2_test_score': array([ 0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.84158416,  0.85148515,  0.84158416,\n",
       "         0.82178218,  0.84158416,  0.85148515,  0.87128713,  0.85148515,\n",
       "         0.87128713,  0.88118812,  0.89108911,  0.89108911,  0.82178218,\n",
       "         0.82178218,  0.81188119,  0.81188119,  0.83168317,  0.84158416,\n",
       "         0.8019802 ,  0.83168317,  0.84158416,  0.82178218,  0.84158416,\n",
       "         0.9009901 ]),\n",
       " 'split2_train_score': array([ 0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.83759733,  0.84204672,  0.85984427,\n",
       "         0.87319244,  0.85205784,  0.86429366,  0.91879867,  0.97107898,\n",
       "         0.89098999,  0.91768632,  0.98442714,  0.99888765,  0.82758621,\n",
       "         0.85094549,  0.86874305,  0.89321468,  0.86206897,  0.90211346,\n",
       "         0.9621802 ,  0.99777531,  0.91101224,  0.95884316,  0.99666296,  1.        ]),\n",
       " 'split3_test_score': array([ 0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,  0.5049505 ,\n",
       "         0.5049505 ,  0.5049505 ,  0.8019802 ,  0.83168317,  0.85148515,\n",
       "         0.85148515,  0.81188119,  0.83168317,  0.84158416,  0.85148515,\n",
       "         0.8019802 ,  0.81188119,  0.87128713,  0.88118812,  0.75247525,\n",
       "         0.78217822,  0.82178218,  0.84158416,  0.82178218,  0.84158416,\n",
       "         0.85148515,  0.85148515,  0.86138614,  0.83168317,  0.89108911,\n",
       "         0.89108911]),\n",
       " 'split3_train_score': array([ 0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.50389321,  0.50389321,  0.50389321,\n",
       "         0.50389321,  0.50389321,  0.83092325,  0.8487208 ,  0.85428254,\n",
       "         0.88097887,  0.84983315,  0.87208009,  0.92213571,  0.96662959,\n",
       "         0.88320356,  0.91101224,  0.97107898,  0.99888765,  0.73971079,\n",
       "         0.84204672,  0.87875417,  0.89210234,  0.88209121,  0.91212458,\n",
       "         0.96662959,  0.99666296,  0.90656285,  0.95661846,  1.        ,  1.        ]),\n",
       " 'split4_test_score': array([ 0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,\n",
       "         0.5 ,  0.5 ,  0.5 ,  0.74,  0.7 ,  0.72,  0.77,  0.75,  0.76,\n",
       "         0.79,  0.81,  0.78,  0.81,  0.8 ,  0.82,  0.73,  0.77,  0.79,\n",
       "         0.75,  0.78,  0.76,  0.81,  0.76,  0.8 ,  0.79,  0.79,  0.81]),\n",
       " 'split4_train_score': array([ 0.50444444,  0.50444444,  0.50444444,  0.50444444,  0.50444444,\n",
       "         0.50444444,  0.50444444,  0.50444444,  0.50444444,  0.50444444,\n",
       "         0.50444444,  0.50444444,  0.83      ,  0.84666667,  0.87      ,\n",
       "         0.88888889,  0.85888889,  0.87666667,  0.92555556,  0.96222222,\n",
       "         0.90777778,  0.93222222,  0.97777778,  1.        ,  0.84666667,\n",
       "         0.86444444,  0.87666667,  0.90111111,  0.86777778,  0.91666667,\n",
       "         0.95888889,  0.99777778,  0.90888889,  0.95333333,  0.99666667,  1.        ]),\n",
       " 'split5_test_score': array([ 0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ,\n",
       "         0.5 ,  0.5 ,  0.5 ,  0.81,  0.81,  0.83,  0.87,  0.82,  0.82,\n",
       "         0.84,  0.79,  0.84,  0.87,  0.86,  0.86,  0.8 ,  0.81,  0.78,\n",
       "         0.8 ,  0.8 ,  0.82,  0.83,  0.79,  0.83,  0.88,  0.84,  0.85]),\n",
       " 'split5_train_score': array([ 0.50444444,  0.50444444,  0.50444444,  0.50444444,  0.50444444,\n",
       "         0.50444444,  0.50444444,  0.50444444,  0.50444444,  0.50444444,\n",
       "         0.50444444,  0.50444444,  0.84      ,  0.83666667,  0.85555556,\n",
       "         0.87555556,  0.85333333,  0.87333333,  0.92111111,  0.97      ,\n",
       "         0.88666667,  0.92555556,  0.97111111,  0.99888889,  0.82444444,\n",
       "         0.83      ,  0.87555556,  0.90222222,  0.88555556,  0.90666667,\n",
       "         0.95777778,  0.99888889,  0.91666667,  0.95555556,  0.99888889,  1.        ]),\n",
       " 'split6_test_score': array([ 0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.82828283,  0.83838384,  0.83838384,\n",
       "         0.83838384,  0.82828283,  0.81818182,  0.90909091,  0.8989899 ,\n",
       "         0.87878788,  0.88888889,  0.8989899 ,  0.8989899 ,  0.84848485,\n",
       "         0.82828283,  0.82828283,  0.7979798 ,  0.85858586,  0.87878788,\n",
       "         0.85858586,  0.8989899 ,  0.86868687,  0.84848485,  0.87878788,\n",
       "         0.88888889]),\n",
       " 'split6_train_score': array([ 0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.83462819,  0.84239734,  0.8590455 ,\n",
       "         0.88124306,  0.85016648,  0.87125416,  0.91897891,  0.96115427,\n",
       "         0.89123196,  0.91231964,  0.96892342,  0.99889012,  0.81798002,\n",
       "         0.85238624,  0.8745838 ,  0.89567148,  0.86792453,  0.89900111,\n",
       "         0.95449501,  0.98890122,  0.89123196,  0.94339623,  0.99889012,  1.        ]),\n",
       " 'split7_test_score': array([ 0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.7979798 ,  0.81818182,  0.80808081,\n",
       "         0.81818182,  0.80808081,  0.83838384,  0.83838384,  0.82828283,\n",
       "         0.83838384,  0.82828283,  0.82828283,  0.83838384,  0.7979798 ,\n",
       "         0.78787879,  0.80808081,  0.80808081,  0.80808081,  0.77777778,\n",
       "         0.80808081,  0.81818182,  0.80808081,  0.81818182,  0.83838384,\n",
       "         0.82828283]),\n",
       " 'split7_train_score': array([ 0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.82685905,  0.83795782,  0.8590455 ,\n",
       "         0.8845727 ,  0.84461709,  0.88124306,  0.93118757,  0.97225305,\n",
       "         0.88568257,  0.91342952,  0.97447281,  1.        ,  0.82019978,\n",
       "         0.84461709,  0.88124306,  0.90566038,  0.86126526,  0.9045505 ,\n",
       "         0.95893452,  0.99778024,  0.91009989,  0.95005549,  0.99889012,  1.        ]),\n",
       " 'split8_test_score': array([ 0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.88888889,  0.90909091,  0.86868687,\n",
       "         0.8989899 ,  0.87878788,  0.88888889,  0.90909091,  0.92929293,\n",
       "         0.87878788,  0.90909091,  0.8989899 ,  0.90909091,  0.87878788,\n",
       "         0.8989899 ,  0.84848485,  0.85858586,  0.86868687,  0.84848485,\n",
       "         0.90909091,  0.85858586,  0.86868687,  0.87878788,  0.8989899 ,\n",
       "         0.8989899 ]),\n",
       " 'split8_train_score': array([ 0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.82352941,  0.84017758,  0.85016648,\n",
       "         0.87680355,  0.84350721,  0.8745838 ,  0.92230855,  0.95449501,\n",
       "         0.89345172,  0.91342952,  0.97558269,  0.99889012,  0.82352941,\n",
       "         0.85016648,  0.88346282,  0.89789123,  0.86459489,  0.90344062,\n",
       "         0.96781354,  0.99778024,  0.91675916,  0.9445061 ,  1.        ,  1.        ]),\n",
       " 'split9_test_score': array([ 0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.50505051,  0.50505051,  0.50505051,\n",
       "         0.50505051,  0.50505051,  0.75757576,  0.78787879,  0.82828283,\n",
       "         0.83838384,  0.83838384,  0.85858586,  0.83838384,  0.84848485,\n",
       "         0.84848485,  0.83838384,  0.83838384,  0.85858586,  0.81818182,\n",
       "         0.7979798 ,  0.80808081,  0.83838384,  0.7979798 ,  0.83838384,\n",
       "         0.81818182,  0.86868687,  0.81818182,  0.80808081,  0.82828283,\n",
       "         0.85858586]),\n",
       " 'split9_train_score': array([ 0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.50388457,  0.50388457,  0.50388457,\n",
       "         0.50388457,  0.50388457,  0.82352941,  0.84683685,  0.84794673,\n",
       "         0.88346282,  0.84572697,  0.87569367,  0.92896781,  0.96448391,\n",
       "         0.89567148,  0.92008879,  0.9800222 ,  0.99889012,  0.8290788 ,\n",
       "         0.86015538,  0.87014428,  0.8945616 ,  0.86903441,  0.90344062,\n",
       "         0.95560488,  0.99334073,  0.90788013,  0.95338513,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([  1.89460072e-04,   8.49326124e-05,   1.64222496e-04,\n",
       "          4.27061703e-04,   5.88403529e-05,   1.45346272e-04,\n",
       "          2.34933559e-04,   6.13054620e-04,   1.97305461e-04,\n",
       "          2.33792205e-04,   6.45767513e-04,   1.15712584e-03,\n",
       "          1.24384369e-04,   6.35463833e-05,   1.20394593e-04,\n",
       "          6.17939564e-05,   8.28963400e-05,   9.19573258e-05,\n",
       "          1.95187884e-04,   2.85835973e-04,   5.70688624e-05,\n",
       "          9.17027438e-05,   2.34280875e-04,   2.30398660e-04,\n",
       "          6.09159983e-05,   1.78061238e-04,   3.74914580e-04,\n",
       "          7.73429659e-04,   1.72195531e-04,   2.36035844e-04,\n",
       "          5.49238871e-04,   1.07189289e-03,   1.37963685e-04,\n",
       "          3.74949098e-04,   2.06279937e-04,   5.58354638e-04]),\n",
       " 'std_score_time': array([  2.47069298e-05,   1.82515457e-05,   2.85655806e-05,\n",
       "          3.72805016e-05,   1.26865571e-05,   3.23788319e-05,\n",
       "          3.81014150e-05,   6.95366251e-05,   3.65491871e-05,\n",
       "          3.93250758e-05,   2.63128686e-05,   6.99041648e-05,\n",
       "          1.77674798e-05,   1.74626107e-05,   3.96817626e-05,\n",
       "          3.51422516e-05,   1.45813185e-05,   1.63352225e-05,\n",
       "          2.31455696e-05,   2.47944318e-05,   1.75480123e-05,\n",
       "          3.21312182e-05,   2.90152962e-05,   1.64771906e-05,\n",
       "          7.75487353e-06,   1.25753314e-05,   4.21706396e-05,\n",
       "          2.01919002e-05,   9.59678465e-06,   1.50461581e-05,\n",
       "          1.22971533e-05,   3.69852387e-05,   2.14370125e-05,\n",
       "          2.56283893e-05,   4.33242772e-05,   3.55725074e-05]),\n",
       " 'std_test_score': array([ 0.0020005 ,  0.0020005 ,  0.0020005 ,  0.0020005 ,  0.0020005 ,\n",
       "         0.0020005 ,  0.0020005 ,  0.0020005 ,  0.0020005 ,  0.0020005 ,\n",
       "         0.0020005 ,  0.0020005 ,  0.06204988,  0.05420794,  0.04514785,\n",
       "         0.04033144,  0.03537089,  0.03834151,  0.03479077,  0.03800794,\n",
       "         0.03214853,  0.0341886 ,  0.03380997,  0.02617608,  0.04199326,\n",
       "         0.03411577,  0.02362217,  0.03480569,  0.03513587,  0.0365302 ,\n",
       "         0.03963014,  0.04524963,  0.03001776,  0.02851471,  0.03105495,\n",
       "         0.02976037]),\n",
       " 'std_train_score': array([ 0.00022225,  0.00022225,  0.00022225,  0.00022225,  0.00022225,\n",
       "         0.00022225,  0.00022225,  0.00022225,  0.00022225,  0.00022225,\n",
       "         0.00022225,  0.00022225,  0.01664047,  0.00448068,  0.00731552,\n",
       "         0.00459428,  0.00433695,  0.00479795,  0.00471295,  0.0052606 ,\n",
       "         0.0074892 ,  0.00742211,  0.0046578 ,  0.00054423,  0.02729025,\n",
       "         0.00925234,  0.00440617,  0.00421359,  0.00946887,  0.00670199,\n",
       "         0.00435959,  0.00285929,  0.00754215,  0.00641501,  0.00121784,  0.        ])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are too many results, we can filter them manually to get the best combination\n",
    "- Note: looking for best parameters is an iterative process. You should start with coarsed-granularity and move to more detailed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy obtained: 0.872\n",
      "Parameters\n",
      "\tlearning_rate: 0.5\n",
      "\tn_estimators: 50\n",
      "\tmax_depth: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Best accuracy obtained: {0}\".format(bst_grid.best_score_))\n",
    "print(\"Parameters\")\n",
    "for key, value in bst_grid.best_params_.items():\n",
    "    print(\"\\t{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Grid-Search\n",
    "when the number of parameters and their values is getting big, the traditional grid-search approach quickly becomes ineffective.\n",
    "- A possible solution might be to randomly pick certain parameters from their distribution. While it's not an exhaustive solution, it's worth giving a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a parameters distribution dictionary:\n",
    "params_dist_grid = { 'max_depth' : [1, 2, 3, 4],\n",
    "                     'gamma' : [0, 0.5, 1],\n",
    "                     'n_estimators' : randint(1, 1001),   # uniform discrete random distribution\n",
    "                     'learning_rate' : uniform,           # gaussain distribution\n",
    "                     'subsample' : uniform(),             # gaussain distribution\n",
    "                     #'colsample_bytree' : uniform()       # gaussain distribution\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ***RandomizedSearchCV*** to randomly pick 10 combinations of parameters. \n",
    "- with this approach you can easily control the number of tested models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=342, shuffle=True),\n",
       "          error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=342, silent=1, subsample=1),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f47a802e978>, 'learning_rate': <scipy.stats._continuous_distns.uniform_gen object at 0x7f47afa20400>, 'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f47a802e630>, 'max_depth': [1, 2, 3, 4], 'gamma': [0, 0.5, 1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=342, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid = RandomizedSearchCV( estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "                              param_distributions=params_dist_grid,\n",
    "                              n_iter=10,\n",
    "                              scoring='accuracy',\n",
    "                              cv = cv,\n",
    "                              random_state=seed)\n",
    "\n",
    "# Train the classifier\n",
    "rs_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some handy properties allowing to quickly analyze best estimator, parameters and obtained score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=1, learning_rate=0.25244657272498761, max_delta_step=0,\n",
       "       max_depth=2, min_child_weight=1, missing=None, n_estimators=541,\n",
       "       nthread=-1, objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=342, silent=1,\n",
       "       subsample=0.47402131267540315)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 1,\n",
       " 'learning_rate': 0.25244657272498761,\n",
       " 'max_depth': 2,\n",
       " 'n_estimators': 541,\n",
       " 'subsample': 0.47402131267540315}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86299999999999999"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_grid.best_score_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
